{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d537d4a",
   "metadata": {},
   "source": [
    "# Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69d2594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# We need to import sparkconf which will set the configurations related to sparkcontext and then use it to create sparkcontext\n",
    "# note that here we are importing those methods from pyspark module not pyspark.sql module unlike sparksession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131a9785",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=SparkConf().setAppName(\"Exam\").setMaster(\"local\")\n",
    "spark= SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa399db1",
   "metadata": {},
   "source": [
    "# Creating RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41b0ae16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Shreyas', 24]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd= spark.parallelize([\"Shreyas\",24,\"Jayaram\", 60])\n",
    "display(myrdd)\n",
    "\n",
    "myrdd.take(2)\n",
    "#using take we can retrive the number of objects we want from an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cb838f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HI', 'MY Name is Shreyas', 'I am unemployed', 'save me']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating RDD from external files\n",
    "#we use textFile method of sparkcontext, this create each line in the text file as a record in RDD.\n",
    "#we can also use this method to import csv and other files, which we will see later.\n",
    "\n",
    "rdd1=spark.textFile('sample3.txt')\n",
    "rdd1.collect()\n",
    "\n",
    "#collect action returns all the data in rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2739a2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/C:/Users/shrey/Music/sample3.txt',\n",
       "  'HI\\nMY Name is Shreyas\\nI am unemployed\\nsave me')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also create RDD which considers all the content in a file as one record using wholeTextFiles method\n",
    "\n",
    "rdd2=spark.wholeTextFiles('sample3.txt')\n",
    "rdd2.collect()\n",
    "\n",
    "#here everytime two records will be created, one is name of the file and second is all the content inside the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e3280",
   "metadata": {},
   "source": [
    "# Actions on RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ddbd152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shreyas'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets look at some basic actions on rdd\n",
    "\n",
    "myrdd.take(3) # gives first 3 records\n",
    "myrdd.collect() # gives all records\n",
    "myrdd.count() # gives count of records in rdd\n",
    "# myrdd.min(), myrdd.max() # used to return the min, max elements in RDD\n",
    "#however in this example where rdd contains both integer and string, min/max will throw error\n",
    "myrdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87c24d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduce - this action will “reduce” an RDD of any kind of value to one value.\n",
    "RDD1 = spark.parallelize([1,3,2,4])\n",
    "add = lambda x,y: x + y\n",
    "RDD1.reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2f1b12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saveAsTextFile - this will save the rdd as text file.with each record in different lines.\n",
    "#myrdd.saveAsTextFile('sample.txt')\n",
    "\n",
    "#getNumPartitions() — returns the number of Partitions\n",
    "myrdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d69f75",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d18eaac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map() — transformation takes in an anonymous function and applies this function to each of the elements in the RDD\n",
    "myrdd = spark.parallelize([1,2,3,4,5])\n",
    "myrdd.map(lambda x:x+1).collect()\n",
    "\n",
    "# or\n",
    "\n",
    "def addone(x):\n",
    "    return x+1\n",
    "myrdd.map(addone).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf034fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi', 'this'], ['is', 'shreyas']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatmap - it is a tranformation that flattens the RDD after applying the function to each element\n",
    "#To flatten means to reduce the dimensionality. In simpler terms, it means reducing a multidimensional to specific dimension.\n",
    "#lets look at working of map on below example\n",
    "\n",
    "rdd1= spark.parallelize([\"Hi this\",\"is shreyas\"])\n",
    "rdd1.map(lambda x: x.split(\" \")).collect()\n",
    "\n",
    "#we can see that map split both of them and gave 2 dimensional rdd, which has two words in each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17f6240a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'this', 'is', 'shreyas']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.flatMap(lambda x: x.split(\" \")).collect()\n",
    "#however we can see that this function has flatten the rdd, that is converted 2d to 1d and returned 4 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47a95599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# repartition(n) — makes n number of partitions on RDD\n",
    "\n",
    "print(rdd1.getNumPartitions())\n",
    "rdd1.repartition(2)\n",
    "print(rdd1.getNumPartitions())\n",
    "\n",
    "#we can see that even after repatitioning we are getting 1 as patitions. \n",
    "#this is because of AQE of spark which sees that data is very less hence no need of patitioning and reduce the partitions to 1.\n",
    "# we can disable the AQE to repartition as per our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef118bd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'conf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# we can disable the AQE to repartition as per our needs\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.adaptive.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute 'conf'"
     ]
    }
   ],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7859330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
