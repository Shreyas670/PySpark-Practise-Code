{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e70689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types._\n",
    "\n",
    "spark=SparkSession.builder.appName(\"DD\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e6afb",
   "metadata": {},
   "source": [
    "We can create a dataframe in multiple ways either using createDataFrame() function, toDF() function or from external files such as csv, json, parquet, etc.\n",
    "Finally, PySpark DataFrame also can be created by reading data from RDBMS Databases and NoSQL databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a2dbf",
   "metadata": {},
   "source": [
    "1. Now let us see how we can create dataframe using a RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc13b0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|    _1|    _2|\n",
      "+------+------+\n",
      "|  Java| 20000|\n",
      "|Python|100000|\n",
      "| Scala|  3000|\n",
      "+------+------+\n",
      "\n",
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let us define the data first\n",
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "\n",
    "#now we will use parallelize function of sparkcontext to create a RDD.\n",
    "rdd= spark.sparkContext.parallelize(data)\n",
    "df = rdd.toDF()\n",
    "df.show()\n",
    "\n",
    "#Now let us give the column names as a parameter to the toDF function.\n",
    "\n",
    "df1=rdd.toDF(columns)\n",
    "df1.show()\n",
    "\n",
    "#The toDF() function takes only one argument that is *cols, which is a multiple number of col elements. \n",
    "#The number of col objects given should be equal to number of columns in the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d64a83",
   "metadata": {},
   "source": [
    "We can also create dataframe from RDD using createDataFrame() method of SparkSession object by giving a list and a schema.\n",
    "Schema can be just columns name of StructType object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4af1e64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n",
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: long (nullable = true)\n",
      "\n",
      "+--------+-----------+\n",
      "|Language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n",
      "root\n",
      " |-- Language: string (nullable = true)\n",
      " |-- users_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# here we have given only column names as schema\n",
    "df2= spark.createDataFrame(rdd,columns)\n",
    "df2.show()\n",
    "df2.printSchema()\n",
    "# lets create a new schema of structType consisting of StructField items.\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "schema = StructType([StructField(\"Language\",StringType(),True),StructField(\"users_count\",IntegerType(),True)])\n",
    "df3=spark.createDataFrame(rdd,schema)\n",
    "df3.show()\n",
    "df3.printSchema()\n",
    "\n",
    "#we can see that users count which was automatically considered as Long by spark is now Integer as we have given it in schema.\n",
    "# StructType takes a list of StructField items as parameters.\n",
    "#StructField takes col name, col types and Nullable as parameters.\n",
    "# if the data does not conform to the defined schema, spark will throw runtime error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8114dd",
   "metadata": {},
   "source": [
    " We can also use createDataFrame to create a new dataframe from a list, row objects etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0016e389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| ID|NAME|\n",
      "+---+----+\n",
      "|  1|   S|\n",
      "|  2|   D|\n",
      "+---+----+\n",
      "\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  1|  S|\n",
      "|  2|  D|\n",
      "+---+---+\n",
      "\n",
      "+---+----+\n",
      "| ID|Name|\n",
      "+---+----+\n",
      "|  1|   S|\n",
      "|  2|   D|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data= [(1,\"S\"),(2,\"D\")]\n",
    "cols=[\"ID\",\"NAME\"]\n",
    "\n",
    "df4=spark.createDataFrame(data,cols)\n",
    "df4.show()\n",
    "\n",
    "#now let us use row object\n",
    "from pyspark.sql import Row\n",
    "data1=[Row(1,\"S\"),Row(2,\"D\")]\n",
    "df5=spark.createDataFrame(data1)\n",
    "df5.show()\n",
    "\n",
    "# we can also create a row object and use it\n",
    "\n",
    "person = Row(\"ID\",\"Name\")\n",
    "data2=[person(1,\"S\"),person(2,\"D\")]\n",
    "df6=spark.createDataFrame(data2)\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48513424",
   "metadata": {},
   "source": [
    "These are the ways we can create a dataframe manually by providing data.\n",
    "However we can create dataframe from source files and DBs, using other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d886ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
