{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0b49358-d619-4cf3-8d5a-092a81599a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"repartition\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "\n",
    "# Set spark conf to skip broadcast join\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "909dd446-70ba-4708-9dbc-062621f310d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_employee = spark.read.option(\"header\",True).option(\"inferSchema\",True).csv(\"employee_records.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6341f65c-508b-4858-8929-f3435b12518f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+-------------------+--------------------+------------------+------+-------------+\n",
      "|first_name|last_name|           job_title|                dob|               email|             phone|salary|department_id|\n",
      "+----------+---------+--------------------+-------------------+--------------------+------------------+------+-------------+\n",
      "|   Richard| Morrison|Public relations ...|1973-05-05 00:00:00|melissagarcia@exa...|     (699)525-4827|512653|            8|\n",
      "|     Bobby| Mccarthy|   Barrister's clerk|1974-04-25 00:00:00|   llara@example.net|(750)846-1602x7458|999836|            7|\n",
      "|    Dennis|   Norman|Land/geomatics su...|1990-06-24 00:00:00| jturner@example.net|  873.820.0518x825|131900|           10|\n",
      "|      John|   Monroe|        Retail buyer|1968-06-16 00:00:00|  erik33@example.net|  820-813-0557x624|485506|            1|\n",
      "|  Michelle|  Elliott|      Air cabin crew|1975-03-31 00:00:00|tiffanyjohnston@e...|     (705)900-5337|604738|            8|\n",
      "+----------+---------+--------------------+-------------------+--------------------+------------------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employee.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47d33e54-1a0c-41d1-9a8c-8cc8fd6c7095",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Current number of partition in the dataframe\n",
    "df_employee.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72d446b2-6a79-4db6-9cb4-8eed94fddc8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|          12|43092|\n",
      "|          22|43025|\n",
      "|           1|43062|\n",
      "|          13|43031|\n",
      "|           6|43014|\n",
      "|          16|43061|\n",
      "|           3|43061|\n",
      "|          20|43031|\n",
      "|           5|43070|\n",
      "|          19|43044|\n",
      "|          15|43058|\n",
      "|           9|43027|\n",
      "|          17|43055|\n",
      "|           4|43046|\n",
      "|           8|43091|\n",
      "|          23| 9828|\n",
      "|           7|43067|\n",
      "|          10|43065|\n",
      "|          21|43022|\n",
      "|          11|43069|\n",
      "|          14|43033|\n",
      "|           2|43054|\n",
      "|           0|43075|\n",
      "|          18|43019|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distribution of records in partitions\n",
    "df_employee.withColumn(\"partition_id\", spark_partition_id()).groupBy(\"partition_id\").count().show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2f82a64-4b6b-45ef-a568-94bebfe25ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dep = spark.read.option(\"header\",True).option(\"inferSchema\",True).csv(\"department_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f3cc8604-c745-4c7e-9c5a-92a1dab8f032",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# join which forces shuffle and created spark.sql.shuffle.partitions number of partitions\n",
    "df_joined =  df_employee.join(df_dep,on=\"department_id\",how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20114ccc-8cfc-472c-af4d-11d1867b99aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+---------+--------------------+-------------------+--------------------+--------------------+------+---------------+--------------------+------------+-----+-------------------+\n",
      "|department_id| first_name|last_name|           job_title|                dob|               email|               phone|salary|department_name|         description|        city|state|            country|\n",
      "+-------------+-----------+---------+--------------------+-------------------+--------------------+--------------------+------+---------------+--------------------+------------+-----+-------------------+\n",
      "|            1|       John|   Monroe|        Retail buyer|1968-06-16 00:00:00|  erik33@example.net|    820-813-0557x624|485506|    Bryan-James|Optimized disinte...|Melissaburgh|   FM|Trinidad and Tobago|\n",
      "|            1|    Rachael|Rodriguez|         Media buyer|1966-12-02 00:00:00|griffinmary@examp...| +1-791-344-7586x548|544732|    Bryan-James|Optimized disinte...|Melissaburgh|   FM|Trinidad and Tobago|\n",
      "|            1|Christopher| Callahan| Exhibition designer|1966-10-23 00:00:00| qwalter@example.com|001-947-745-3939x...|251057|    Bryan-James|Optimized disinte...|Melissaburgh|   FM|Trinidad and Tobago|\n",
      "|            1|    Lindsey|   Huerta|Embryologist, cli...|1964-10-20 00:00:00|  psmith@example.net|   527.934.6665x1378|878257|    Bryan-James|Optimized disinte...|Melissaburgh|   FM|Trinidad and Tobago|\n",
      "|            1|      David|   Harris|   Company secretary|1990-04-13 00:00:00|     nli@example.com|001-959-766-1180x...|249553|    Bryan-James|Optimized disinte...|Melissaburgh|   FM|Trinidad and Tobago|\n",
      "+-------------+-----------+---------+--------------------+-------------------+--------------------+--------------------+------+---------------+--------------------+------------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "815b484e-cb08-4c3f-be43-f66245b7c601",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default 200 partitions after shuffle\n",
    "df_joined.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d5bc767-53f0-4447-b61d-b327217a8d5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|partition_id| count|\n",
      "+------------+------+\n",
      "|         103|100417|\n",
      "|         122| 99780|\n",
      "|          43| 99451|\n",
      "|         107| 99805|\n",
      "|          49| 99706|\n",
      "|          51|100248|\n",
      "|         102|100214|\n",
      "|          66|100210|\n",
      "|         174|100155|\n",
      "|          89|100014|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Due to large num of partitions, uneven distrbution and lot of empty partitions\n",
    "df_joined.withColumn(\"partition_id\", spark_partition_id()).groupBy(\"partition_id\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff6f5b5d-bd56-4222-a28b-b78e14570384",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|          12|50000|\n",
      "|           1|49998|\n",
      "|          13|50000|\n",
      "|           6|50002|\n",
      "|          16|50000|\n",
      "|           3|49999|\n",
      "|           5|50002|\n",
      "|          19|49999|\n",
      "|          15|50000|\n",
      "|           9|50000|\n",
      "|          17|50001|\n",
      "|           4|50001|\n",
      "|           8|50000|\n",
      "|           7|50001|\n",
      "|          10|50001|\n",
      "|          11|50000|\n",
      "|          14|50000|\n",
      "|           2|49998|\n",
      "|           0|49999|\n",
      "|          18|49999|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# repartition the data to achieve uniformity\n",
    "df_rep = df_joined.repartition(20)\n",
    "df_rep.withColumn(\"partition_id\", spark_partition_id()).groupBy(\"partition_id\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "80db851a-c678-486c-81e8-f1ee916a6eab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|partition_id| count|\n",
      "+------------+------+\n",
      "|           1|155967|\n",
      "|           6| 95853|\n",
      "|           3| 90682|\n",
      "|           5| 89830|\n",
      "|           9|105484|\n",
      "|           4| 96771|\n",
      "|           8| 89842|\n",
      "|           7| 76695|\n",
      "|           2|100432|\n",
      "|           0| 98444|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets try to perform repartition on a particular column\n",
    "df_names = df_rep.repartition(10,\"first_name\")\n",
    "df_names.withColumn(\"partition_id\", spark_partition_id()).groupBy(\"partition_id\").count().show(30)\n",
    "\n",
    "# we can see that it now the data is not uniformly distributed. So when we give a key to perform repartition, it can lead to data skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "33760b8e-a4f1-4459-9328-e6d8ee34c4a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|partition_id| count|\n",
      "+------------+------+\n",
      "|           1|191114|\n",
      "|           3|172548|\n",
      "|           4|195326|\n",
      "|           2|186601|\n",
      "|           0|254411|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_coal = df_names.coalesce(5)\n",
    "df_coal.withColumn(\"partition_id\", spark_partition_id()).groupBy(\"partition_id\").count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e8efa594-454c-4434-bc44-9eb431f59da5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Coalesce 5\n",
      "+- Exchange hashpartitioning(first_name#343, 10), REPARTITION_BY_NUM, [id=#2998]\n",
      "   +- *(5) Project [department_id#350, first_name#343, last_name#344, job_title#345, dob#346, email#347, phone#348, salary#349, department_name#494, description#495, city#496, state#497, country#498]\n",
      "      +- *(5) SortMergeJoin [department_id#350], [department_id#493], Inner\n",
      "         :- *(2) Sort [department_id#350 ASC NULLS FIRST], false, 0\n",
      "         :  +- Exchange hashpartitioning(department_id#350, 200), ENSURE_REQUIREMENTS, [id=#2982]\n",
      "         :     +- *(1) Filter isnotnull(department_id#350)\n",
      "         :        +- FileScan csv [first_name#343,last_name#344,job_title#345,dob#346,email#347,phone#348,salary#349,department_id#350] Batched: false, DataFilters: [isnotnull(department_id#350)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/data/employee_records.txt], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:timestamp,email:string,phone:strin...\n",
      "         +- *(4) Sort [department_id#493 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(department_id#493, 200), ENSURE_REQUIREMENTS, [id=#2990]\n",
      "               +- *(3) Filter isnotnull(department_id#493)\n",
      "                  +- FileScan csv [department_id#493,department_name#494,description#495,city#496,state#497,country#498] Batched: false, DataFilters: [isnotnull(department_id#493)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/data/department_data.txt], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_coal.explain()\n",
    "# we can see that coalesce did not perform shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc7e0d5e-3d41-4a17-832e-208a2533a45e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(6) HashAggregate(keys=[first_name#343], functions=[sum(salary#349)])\n",
      "+- *(6) HashAggregate(keys=[first_name#343], functions=[partial_sum(salary#349)])\n",
      "   +- Exchange hashpartitioning(first_name#343, 10), REPARTITION_BY_NUM, [id=#3099]\n",
      "      +- *(5) Project [first_name#343, salary#349]\n",
      "         +- *(5) SortMergeJoin [department_id#350], [department_id#493], Inner\n",
      "            :- *(2) Sort [department_id#350 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(department_id#350, 200), ENSURE_REQUIREMENTS, [id=#3083]\n",
      "            :     +- *(1) Filter isnotnull(department_id#350)\n",
      "            :        +- FileScan csv [first_name#343,salary#349,department_id#350] Batched: false, DataFilters: [isnotnull(department_id#350)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/data/employee_records.txt], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<first_name:string,salary:int,department_id:int>\n",
      "            +- *(4) Sort [department_id#493 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(department_id#493, 200), ENSURE_REQUIREMENTS, [id=#3091]\n",
      "                  +- *(3) Filter isnotnull(department_id#493)\n",
      "                     +- FileScan csv [department_id#493] Batched: false, DataFilters: [isnotnull(department_id#493)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/data/department_data.txt], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets try aggregation on data which is partitioned by a column\n",
    "df_names_agg = df_names.groupBy(\"first_name\").sum(\"salary\")\n",
    "df_names_agg.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "49269e6d-bbb0-4bbd-a074-e0ed2924512d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(6) HashAggregate(keys=[first_name#343], functions=[sum(salary#349)])\n",
      "+- Exchange hashpartitioning(first_name#343, 200), ENSURE_REQUIREMENTS, [id=#3200]\n",
      "   +- *(5) HashAggregate(keys=[first_name#343], functions=[partial_sum(salary#349)])\n",
      "      +- *(5) Project [first_name#343, salary#349]\n",
      "         +- *(5) SortMergeJoin [department_id#350], [department_id#493], Inner\n",
      "            :- *(2) Sort [department_id#350 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(department_id#350, 200), ENSURE_REQUIREMENTS, [id=#3183]\n",
      "            :     +- *(1) Filter isnotnull(department_id#350)\n",
      "            :        +- FileScan csv [first_name#343,salary#349,department_id#350] Batched: false, DataFilters: [isnotnull(department_id#350)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/data/employee_records.txt], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<first_name:string,salary:int,department_id:int>\n",
      "            +- *(4) Sort [department_id#493 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(department_id#493, 200), ENSURE_REQUIREMENTS, [id=#3191]\n",
      "                  +- *(3) Filter isnotnull(department_id#493)\n",
      "                     +- FileScan csv [department_id#493] Batched: false, DataFilters: [isnotnull(department_id#493)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/data/department_data.txt], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets try same aggregation on data not partitioned by that column\n",
    "df_joined_agg = df_joined.groupBy(\"first_name\").sum(\"salary\")\n",
    "df_joined_agg.explain()\n",
    "# we can see there is a Exchange hashpartitioning in the second line now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
