{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff547f6-7dbc-4d13-a044-24bdf4a8f540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"bucketing\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2a5e6e2-73d6-4b3c-aee0-87d510e324e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a259bef-6517-4716-8b19-89129410e793",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+--------+-------------------+------------+\n",
      "|order_id|product_id|customer_id|quantity|order_date         |total_amount|\n",
      "+--------+----------+-----------+--------+-------------------+------------+\n",
      "|1       |80        |10         |4       |2023-03-20 00:00:00|1003        |\n",
      "|2       |69        |30         |3       |2023-12-11 00:00:00|780         |\n",
      "|3       |61        |20         |4       |2023-04-26 00:00:00|1218        |\n",
      "|4       |62        |44         |3       |2023-08-26 00:00:00|2022        |\n",
      "|5       |78        |46         |4       |2023-08-05 00:00:00|1291        |\n",
      "+--------+----------+-----------+--------+-------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders = spark.read.option(\"header\",True).option(\"inferSchema\",True).csv(\"orders.csv\")\n",
    "orders.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cc80140-02f7-483d-a3cd-39dc0c8157a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-------+-----+-----+\n",
      "|product_id|product_name|category   |brand  |price|stock|\n",
      "+----------+------------+-----------+-------+-----+-----+\n",
      "|1         |Product_1   |Electronics|Brand_4|26   |505  |\n",
      "|2         |Product_2   |Apparel    |Brand_4|489  |15   |\n",
      "|3         |Product_3   |Apparel    |Brand_4|102  |370  |\n",
      "|4         |Product_4   |Groceries  |Brand_1|47   |433  |\n",
      "|5         |Product_5   |Groceries  |Brand_3|244  |902  |\n",
      "+----------+------------+-----------+-------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products = spark.read.option(\"header\",True).option(\"inferSchema\",True).csv(\"products.csv\")\n",
    "products.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c7a442c-5d17-42ac-bd23-c7bb1619f09b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [product_id#18], [product_id#78], Inner\n",
      "   :- Sort [product_id#18 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(product_id#18, 200), ENSURE_REQUIREMENTS, [id=#84]\n",
      "   :     +- Filter isnotnull(product_id#18)\n",
      "   :        +- FileScan csv [order_id#17,product_id#18,customer_id#19,quantity#20,order_date#21,total_amount#22] Batched: false, DataFilters: [isnotnull(product_id#18)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/data/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<order_id:int,product_id:int,customer_id:int,quantity:int,order_date:timestamp,total_amount...\n",
      "   +- Sort [product_id#78 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(product_id#78, 200), ENSURE_REQUIREMENTS, [id=#85]\n",
      "         +- Filter isnotnull(product_id#78)\n",
      "            +- FileScan csv [product_id#78,product_name#79,category#80,brand#81,price#82,stock#83] Batched: false, DataFilters: [isnotnull(product_id#78)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/data/products.csv], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:int,product_name:string,category:string,brand:string,price:int,stock:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join without bucketing\n",
    "products_orders = orders.join(products,orders.product_id == products.product_id,\"inner\")\n",
    "products_orders.explain()\n",
    "\n",
    "# we can see in the plan that shuffle sort merge is taking place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f3771c-84de-41d2-9cb7-886a2e207409",
   "metadata": {},
   "source": [
    "# Bucketing\n",
    "Now lets create buckets on both the data and store it as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "964de98e-0fb5-4232-becb-fcf6cc9b47f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.write.bucketBy(4, col = \"product_id\").mode(\"overwrite\").saveAsTable(\"orders_bucketed\")\n",
    "# After this we can see that there is a folder created called orders_bucketed with 4 files inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c9fa9cb-0442-4fa6-bfca-967563e2e66b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "products.write.bucketBy(4, col = \"product_id\").mode(\"overwrite\").saveAsTable(\"products_bucketed\")\n",
    "# Similarly another folder is created for products_bucketed with 4 files inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "979dc5eb-ea68-44f1-b699-74910e384511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now lets read this saved data as table in spark\n",
    "# As we did saveAsTable, the metadata of this will be stored in spark catalog, so when we can directly read it as tables in spark\n",
    "# If we did not saveAsTable, then this can only be read as dataframe and not tables\n",
    "\n",
    "df_orders_bucketed = spark.table(\"orders_bucketed\")\n",
    "df_products_bucketed = spark.table(\"products_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bbf91a4-b8b4-42cf-8bc2-d6de991c1a14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [product_id#231, order_id#230, customer_id#232, quantity#233, order_date#234, total_amount#235, product_name#243, category#244, brand#245, price#246, stock#247]\n",
      "   +- SortMergeJoin [product_id#231], [product_id#242], Inner\n",
      "      :- Sort [product_id#231 ASC NULLS FIRST], false, 0\n",
      "      :  +- Filter isnotnull(product_id#231)\n",
      "      :     +- FileScan parquet default.orders_bucketed[order_id#230,product_id#231,customer_id#232,quantity#233,order_date#234,total_amount#235] Batched: true, Bucketed: true, DataFilters: [isnotnull(product_id#231)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/data/spark-warehouse/orders_bucketed], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<order_id:int,product_id:int,customer_id:int,quantity:int,order_date:timestamp,total_amount..., SelectedBucketsCount: 4 out of 4\n",
      "      +- Sort [product_id#242 ASC NULLS FIRST], false, 0\n",
      "         +- Filter isnotnull(product_id#242)\n",
      "            +- FileScan parquet default.products_bucketed[product_id#242,product_name#243,category#244,brand#245,price#246,stock#247] Batched: true, Bucketed: true, DataFilters: [isnotnull(product_id#242)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/data/spark-warehouse/products_bucketed], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:int,product_name:string,category:string,brand:string,price:int,stock:int>, SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_products_orders_joined =  df_orders_bucketed.join(df_products_bucketed, on = \"product_id\", how=\"inner\")\n",
    "df_products_orders_joined.explain()\n",
    "# we can see that there is sortMerge join taking place, but without shuffle being done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead60e9-a8ec-417e-8d6c-343450f25d23",
   "metadata": {},
   "source": [
    "# Bucketing In Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "187ef4c3-e2b7-449f-b296-f009ed85f4fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[product_id#18], functions=[sum(total_amount#22)])\n",
      "   +- Exchange hashpartitioning(product_id#18, 200), ENSURE_REQUIREMENTS, [id=#158]\n",
      "      +- HashAggregate(keys=[product_id#18], functions=[partial_sum(total_amount#22)])\n",
      "         +- FileScan csv [product_id#18,total_amount#22] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/data/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,total_amount:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Groupby without bucketing\n",
    "df_product_sales = (\n",
    "    orders\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(F.sum(\"total_amount\").alias(\"sales\"))\n",
    ")\n",
    "\n",
    "df_product_sales.explain()\n",
    "# we can see shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb8287e4-e1e2-4914-8521-c46607d0ead6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[product_id#231], functions=[sum(total_amount#235)])\n",
      "   +- HashAggregate(keys=[product_id#231], functions=[partial_sum(total_amount#235)])\n",
      "      +- FileScan parquet default.orders_bucketed[product_id#231,total_amount#235] Batched: true, Bucketed: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/data/spark-warehouse/orders_bucketed], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,total_amount:int>, SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Groupby without bucketing\n",
    "df_product_sales = (\n",
    "    df_orders_bucketed\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(F.sum(\"total_amount\").alias(\"sales\"))\n",
    ")\n",
    "\n",
    "df_product_sales.explain()\n",
    "# There is no shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38948672-13e3-41c2-90e9-d81aa832f69b",
   "metadata": {},
   "source": [
    "# Bucket pruning (Filters)\n",
    "Bucketing is also helpful when we filer the data, if we filter the data based on id 100, it will perform hash on 100 and whatever the output is perform a mod (no_of_buckets) and based on output it will directly just read that bucket instead of full scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab14f04d-612d-431f-9b07-959beab41f24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[product_id#231], functions=[sum(total_amount#235)])\n",
      "   +- HashAggregate(keys=[product_id#231], functions=[partial_sum(total_amount#235)])\n",
      "      +- Filter (isnotnull(product_id#231) AND (product_id#231 = 1))\n",
      "         +- FileScan parquet default.orders_bucketed[product_id#231,total_amount#235] Batched: true, Bucketed: true, DataFilters: [isnotnull(product_id#231), (product_id#231 = 1)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/data/spark-warehouse/orders_bucketed], PartitionFilters: [], PushedFilters: [IsNotNull(product_id), EqualTo(product_id,1)], ReadSchema: struct<product_id:int,total_amount:int>, SelectedBucketsCount: 1 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_product_sales_bucket_pruning = (\n",
    "    df_orders_bucketed\n",
    "    .filter(F.col(\"product_id\") == 1)\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(F.sum(\"total_amount\").alias(\"sales\"))\n",
    ")\n",
    "df_product_sales_bucket_pruning.explain()\n",
    "# We can see that SelectedBucketsCount: 1 out of 4 in the last line of plan which tells us that only one bucket is searched for id =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02acc853-568f-41e9-8d3a-fcd60f4f3768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
