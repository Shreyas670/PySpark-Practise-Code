{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9633b25-365e-405a-a0a5-f9b079b62e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"broadcast\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e79d38c-42dd-4c98-978c-d4c64d4ececb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read EMP CSV data\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "df = spark.read.schema(_schema).option(\"header\",True).csv('employee_records.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f1f403e-8145-4378-90bc-71b62c8d2ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dictionary used to lookup on department names\n",
    "dept_names = {1 : 'Department 1', \n",
    "              2 : 'Department 2', \n",
    "              3 : 'Department 3', \n",
    "              4 : 'Department 4',\n",
    "              5 : 'Department 5', \n",
    "              6 : 'Department 6', \n",
    "              7 : 'Department 7', \n",
    "              8 : 'Department 8', \n",
    "              9 : 'Department 9', \n",
    "              10 : 'Department 10'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07098c09-84ba-47b6-8471-5b546658ee56",
   "metadata": {},
   "source": [
    "Now if we want to use this dictionary to perform a lookup on employee df, then for each partitions, for each task this dictionary will be sent from driver to executors.                                                               \n",
    "However, we can broadcast this to all executors beforehand, so that it will only be sent once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66c21a62-7166-430b-9cf5-b115beb0b936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Broadcast the variable\n",
    "broadcast_depnames = spark.sparkContext.broadcast(dept_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06df6308-3ae0-4a9f-8f1d-ea2f47ed547d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.broadcast.Broadcast"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can check the type of this variable, which will not be dict because now it is not just a python object it is pyspark broadcast variable\n",
    "type(broadcast_depnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a814789-b979-4380-89ed-0e033e058b99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Department 1',\n",
       " 2: 'Department 2',\n",
       " 3: 'Department 3',\n",
       " 4: 'Department 4',\n",
       " 5: 'Department 5',\n",
       " 6: 'Department 6',\n",
       " 7: 'Department 7',\n",
       " 8: 'Department 8',\n",
       " 9: 'Department 9',\n",
       " 10: 'Department 10'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check value of a broadcast variable we can use .value\n",
    "broadcast_depnames.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "712c8c13-6d5e-4891-b4b7-8a41d8089734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now to perform the lookup, we need to write a function which receives the deparment id and returns the department name. As this is not a dataframe, we cant join and need to use a udf\n",
    "\n",
    "# Create UDF to return Department name\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def get_dept_names(dep_id):\n",
    "    return broadcast_depnames.value.get(dep_id)\n",
    "    \n",
    "get_dept_names_udf = udf(get_dept_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6209332-09b8-4ba0-88b9-e051f5f8995b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use udf to perform lookup\n",
    "final_df = df.withColumn(\"dep_name\",get_dept_names_udf(df.department_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb064c7d-32f8-4419-948c-25954f8a8c29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+-------------+\n",
      "|first_name| last_name|           job_title|       dob|               email|               phone|  salary|department_id|     dep_name|\n",
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+-------------+\n",
      "|   Richard|  Morrison|Public relations ...|1973-05-05|melissagarcia@exa...|       (699)525-4827|512653.0|            8| Department 8|\n",
      "|     Bobby|  Mccarthy|   Barrister's clerk|1974-04-25|   llara@example.net|  (750)846-1602x7458|999836.0|            7| Department 7|\n",
      "|    Dennis|    Norman|Land/geomatics su...|1990-06-24| jturner@example.net|    873.820.0518x825|131900.0|           10|Department 10|\n",
      "|      John|    Monroe|        Retail buyer|1968-06-16|  erik33@example.net|    820-813-0557x624|485506.0|            1| Department 1|\n",
      "|  Michelle|   Elliott|      Air cabin crew|1975-03-31|tiffanyjohnston@e...|       (705)900-5337|604738.0|            8| Department 8|\n",
      "|    Ashley|   Montoya|        Cartographer|1976-01-16|patrickalexandra@...|        211.440.5466|483339.0|            6| Department 6|\n",
      "| Nathaniel|     Smith|     Quality manager|1985-06-28|  lori44@example.net|        936-403-3179|419644.0|            7| Department 7|\n",
      "|     Faith|  Cummings|Industrial/produc...|1978-07-01| ygordon@example.org|       (889)246-5588|205939.0|            7| Department 7|\n",
      "|  Margaret|    Sutton|Administrator, ed...|1975-08-16| diana44@example.net|001-647-530-5036x...|671167.0|            8| Department 8|\n",
      "|      Mary|    Sutton|   Freight forwarder|1979-12-28|  ryan36@example.com|   422.562.7254x3159|993829.0|            7| Department 7|\n",
      "|      Jake|      King|       Lexicographer|1994-07-11|monica93@example.org|+1-535-652-9715x6...|702101.0|            4| Department 4|\n",
      "|   Heather|     Haley|         Music tutor|1981-06-01|stephanie65@examp...|   (652)815-7973x298|570960.0|            6| Department 6|\n",
      "|    Thomas|    Thomas|Chartered managem...|2001-07-17|pwilliams@example...|001-245-848-0028x...|339441.0|            6| Department 6|\n",
      "|   Leonard|   Carlson|       Art therapist|1990-10-18|gabrielmurray@exa...|          9247590563|469728.0|            8| Department 8|\n",
      "|      Mark|      Wood|   Market researcher|1963-10-13|nicholas76@exampl...|   311.439.1606x3342|582291.0|            4| Department 4|\n",
      "|    Tracey|Washington|Travel agency man...|1986-05-07|  mark07@example.com|    001-912-206-6456|146456.0|            4| Department 4|\n",
      "|   Rachael| Rodriguez|         Media buyer|1966-12-02|griffinmary@examp...| +1-791-344-7586x548|544732.0|            1| Department 1|\n",
      "|      Tara|       Liu|   Financial adviser|1998-10-12|alexandraobrien@e...|        216.696.6061|399503.0|            3| Department 3|\n",
      "|       Ana|    Joseph|      Retail manager|1995-01-10|  rmorse@example.org|  (726)363-7526x9965|761988.0|           10|Department 10|\n",
      "|   Richard|      Hall|Engineer, civil (...|1967-03-02|brandoncardenas@e...| (964)451-9007x22496|660659.0|            4| Department 4|\n",
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can see that dep name is populated and there was no shuffle involved\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc5c20-48f0-4782-ac96-916c111f4773",
   "metadata": {},
   "source": [
    "# Accumulators\n",
    "Lets see an example of how accumulators can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f6aff92-b940-439b-9c0c-dcb65e6d522b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------------+\n",
      "|department_id|CAST(sum(salary) AS BIGINT)|\n",
      "+-------------+---------------------------+\n",
      "|            6|                50294510721|\n",
      "+-------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate total salary of deparment 6\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "final_df.where(\"department_id=6\").groupBy(\"department_id\").agg(sum(\"salary\").cast(\"long\")).show()\n",
    "\n",
    "# If we ue the above way to get the output there will be a shuffle involved and deparment id 6 records will be distributed across executors so they will need to be brought together to perform aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21f4ab8d-d6a3-427d-a407-3d714db57643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets see how can we avoid the shuffle by using accumulators\n",
    "\n",
    "# We need to initialize the accumulator with a initial value\n",
    "dept_sal = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc621d43-2924-4d71-aec6-02bcb5dd4d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As we don't want to use group by which forces a shuffle, we need to use udf to calculate the output and we need to run it on all rows present in the dataframe.\n",
    "\n",
    "def calculate_salary(department_id, salary):\n",
    "    if department_id==6:\n",
    "        dept_sal.add(salary)\n",
    "\n",
    "# foreach function runs a given function on each row of the dataframe\n",
    "df.where(\"department_id=6\").foreach(lambda row: calculate_salary(row.department_id, row.salary))\n",
    "\n",
    "#Now we can see that using accumulator there was no shuffle involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96a8a951-2d35-4dc5-a423-b87d486d2321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50294510721.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the value of the accumulator we can use .value\n",
    "dept_sal.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45a2d25e-09f8-4e99-8609-35c57ce89327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc71ba5a-43da-49b3-b70c-96842c870afb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
