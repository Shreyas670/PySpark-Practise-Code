{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "616fed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName(\"sql\").master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd71bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| Id|   Name|\n",
      "+---+-------+\n",
      "|  1|Shreyas|\n",
      "|  2|jayaram|\n",
      "+---+-------+\n",
      "\n",
      "+---+---+\n",
      "| Id|Age|\n",
      "+---+---+\n",
      "|  1| 20|\n",
      "|  3| 10|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# joins can be done on two dataframes having similar columns. There are many types of joins. \n",
    "# join usually has the beow syntax\n",
    "#  dataframe1.join(dataframe2,dataframre1.colname==dataframe2.colname,\"type\")\n",
    "# this will return a new dataframe with data joined.\n",
    "\n",
    "data1=[(1, \"Shreyas\"),(2,\"jayaram\")]\n",
    "schema1=[\"Id\",\"Name\"]\n",
    "data2=[(1,20),(3,10)]\n",
    "schema2=[\"Id\",\"Age\"]\n",
    "df1=spark.createDataFrame(data1,schema1)\n",
    "df2=spark.createDataFrame(data2,schema2)\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ace559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+---+\n",
      "| Id|   Name| Id|Age|\n",
      "+---+-------+---+---+\n",
      "|  1|Shreyas|  1| 20|\n",
      "+---+-------+---+---+\n",
      "\n",
      "+----+-------+----+----+\n",
      "|  Id|   Name|  Id| Age|\n",
      "+----+-------+----+----+\n",
      "|   1|Shreyas|   1|  20|\n",
      "|   2|jayaram|NULL|NULL|\n",
      "|NULL|   NULL|   3|  10|\n",
      "+----+-------+----+----+\n",
      "\n",
      "+---+-------+----+----+\n",
      "| Id|   Name|  Id| Age|\n",
      "+---+-------+----+----+\n",
      "|  1|Shreyas|   1|  20|\n",
      "|  2|jayaram|NULL|NULL|\n",
      "+---+-------+----+----+\n",
      "\n",
      "+----+-------+---+---+\n",
      "|  Id|   Name| Id|Age|\n",
      "+----+-------+---+---+\n",
      "|   1|Shreyas|  1| 20|\n",
      "|NULL|   NULL|  3| 10|\n",
      "+----+-------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.Id==df2.Id,\"Inner\").show()\n",
    "df1.join(df2,df1.Id==df2.Id,\"Outer\").show()\n",
    "df1.join(df2,df1.Id==df2.Id,\"Left\" ).show()  # Left_outer can aLso be used\n",
    "df1.join(df2,df1.Id==df2.Id,\"Right\" ).show() # right_outer can aLso be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdff0c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+----+\n",
      "| Id|   Name|  Id| Age|\n",
      "+---+-------+----+----+\n",
      "|  1|Shreyas|   1|  20|\n",
      "|  2|jayaram|NULL|NULL|\n",
      "+---+-------+----+----+\n",
      "\n",
      "+---+-------+----+----+\n",
      "| Id|   Name|  Id| Age|\n",
      "+---+-------+----+----+\n",
      "|  1|Shreyas|   1|  20|\n",
      "|  2|jayaram|NULL|NULL|\n",
      "+---+-------+----+----+\n",
      "\n",
      "+---+-------+---+---+\n",
      "| Id|   Name| Id|Age|\n",
      "+---+-------+---+---+\n",
      "|  1|Shreyas|  1| 20|\n",
      "|  1|Shreyas|  3| 10|\n",
      "|  2|jayaram|  1| 20|\n",
      "|  2|jayaram|  3| 10|\n",
      "+---+-------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now lets see semi and anti joins\n",
    "df1.join(df2,df1.Id==df2.Id,\"Left\" ).show() # It will consider the data only from Left dataframe which did not match with other dataframe.\n",
    "df1.join(df2,df1.Id==df2.Id,\"Left\" ).show() # opposite to semi join, It will consider the data only from Left dataframe which matches with other dataframe.\n",
    "\n",
    "df1.crossJoin(df2).show() # It will join all rows in one datafrane with all rows in another. Here there is no join condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee027387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| Id|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| Id|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of self join\n",
    "#df1.alias(\"Table1\").join(df1.alias(\"Table2\"),col(Table1.Id)==col(Table2.Id),\"Inner\")\n",
    "# As we are using same datafnane, we have to give alias name.\n",
    "\n",
    "#Complications with join.\n",
    "#one of the Complication with join is that when we join tables hav ng same column names for key columns, then output dataframe will have two coLunn names some.\n",
    "# Then when we try to access that columnn, it will throw an error as there are two columns with same name.\n",
    "#To resolve this we can either drop one of the column after joining or rename the columnn before joining.\n",
    "from pyspark.sql.functions import *\n",
    "df1.join(df2, df1.Id==df2.Id, \"Inner\").drop(df1.Id).select(\"Id\").show()\n",
    "\n",
    "df3=df1.withColumnRenamed(\"Id\",\"Id_1\")\n",
    "df3.join(df2, df3.Id_1==df2.Id,\"Inner\").select(\"Id\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba92d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
