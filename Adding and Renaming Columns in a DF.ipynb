{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c3b42e",
   "metadata": {},
   "source": [
    "# withColumn() Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbcf1d0",
   "metadata": {},
   "source": [
    "PySpark withColumn() is a transformation function of DataFrame which is used to change the value, convert the datatype of an existing column, create a new column, and many more.\n",
    "\n",
    "withColumn() takes two arguments first one is column name and second is a col expression for the new or modified col.\n",
    "\n",
    "Lets look at how we can change the values of a column using this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717f1243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName(\"Cols\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc0ee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "\n",
    "df= spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f10cab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M| 30000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M| 40000|\n",
      "|   Robert|          |Williams|1978-09-05|     M| 40000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F| 40000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|   -10|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lets modify data in a column\n",
    "\n",
    "df1=df.withColumn(\"salary\",df.salary*10) # can also be written as col(salary)*10\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a6a51ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lets change the data type of a column\n",
    "df1.printSchema()\n",
    "\n",
    "# to change the datatype we use cast method which is availble for column objects\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "df1.withColumn(\"salary\",df1.salary.cast(\"Integer\")).printSchema()\n",
    "df1.withColumn(\"salary\",col(\"salary\").cast(StringType())).printSchema()\n",
    "\n",
    "# We can refer to columns using dataframe.colname or col('colname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b82f6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+---------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|NewSalary|\n",
      "+---------+----------+--------+----------+------+------+---------+\n",
      "|    James|          |   Smith|1991-04-01|     M| 30000|    29990|\n",
      "|  Michael|      Rose|        |2000-05-19|     M| 40000|    39990|\n",
      "|   Robert|          |Williams|1978-09-05|     M| 40000|    39990|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F| 40000|    39990|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|   -10|      -20|\n",
      "+---------+----------+--------+----------+------+------+---------+\n",
      "\n",
      "+---------+----------+--------+----------+------+------+-------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|Country|\n",
      "+---------+----------+--------+----------+------+------+-------+\n",
      "|    James|          |   Smith|1991-04-01|     M| 30000|  India|\n",
      "|  Michael|      Rose|        |2000-05-19|     M| 40000|  India|\n",
      "|   Robert|          |Williams|1978-09-05|     M| 40000|  India|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F| 40000|  India|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|   -10|  India|\n",
      "+---------+----------+--------+----------+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now lets look at creating new column\n",
    "# we can create new column either by modifying existing column or using a constant value across the column with lit function.\n",
    "from pyspark.sql.functions import lit\n",
    "df3= df1.withColumn(\"NewSalary\",col('salary')-10)\n",
    "df3.show()\n",
    "\n",
    "df4= df1.withColumn(\"Country\", lit(\"India\"))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dde1502",
   "metadata": {},
   "source": [
    "We saw that using withColumn() we can modify existing columns and create new ones. The function checks the column name provided in the argument, if a column with that name exists in the dataframe then it will modify that column according to the logic provided or else it will create a new column with that logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c47366",
   "metadata": {},
   "source": [
    "# withColumnRenamed() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ffc49",
   "metadata": {},
   "source": [
    "This function takes two parameters; the first is your existing column name and the second is the new column name you wish for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b44389b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-------------+------+------+\n",
      "|firstname|middlename|lastname|Date of Birth|gender|salary|\n",
      "+---------+----------+--------+-------------+------+------+\n",
      "|    James|          |   Smith|   1991-04-01|     M| 30000|\n",
      "|  Michael|      Rose|        |   2000-05-19|     M| 40000|\n",
      "|   Robert|          |Williams|   1978-09-05|     M| 40000|\n",
      "|    Maria|      Anne|   Jones|   1967-12-01|     F| 40000|\n",
      "|      Jen|      Mary|   Brown|   1980-02-17|     F|   -10|\n",
      "+---------+----------+--------+-------------+------+------+\n",
      "\n"
     ]
    },
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_ITERABLE] Column is not iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df1\u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdob\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate of Birth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m----> 2\u001b[0m df1\u001b[38;5;241m.\u001b[39mwithColumnRenamed(df1\u001b[38;5;241m.\u001b[39mdob,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate of Birth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\spark\\python\\pyspark\\sql\\dataframe.py:5204\u001b[0m, in \u001b[0;36mDataFrame.withColumnRenamed\u001b[1;34m(self, existing, new)\u001b[0m\n\u001b[0;32m   5172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwithColumnRenamed\u001b[39m(\u001b[38;5;28mself\u001b[39m, existing: \u001b[38;5;28mstr\u001b[39m, new: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   5173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` by renaming an existing column.\u001b[39;00m\n\u001b[0;32m   5174\u001b[0m \u001b[38;5;124;03m    This is a no-op if the schema doesn't contain the given column name.\u001b[39;00m\n\u001b[0;32m   5175\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5202\u001b[0m \u001b[38;5;124;03m    +----+-----+\u001b[39;00m\n\u001b[0;32m   5203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mwithColumnRenamed(existing, new), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mc:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1314\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m-> 1314\u001b[0m     args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m     command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m         args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m         proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mc:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1277\u001b[0m, in \u001b[0;36mJavaMember._build_args\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverters) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1277\u001b[0m         (new_args, temp_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_args(args)\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1279\u001b[0m         new_args \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[1;32mc:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1264\u001b[0m, in \u001b[0;36mJavaMember._get_args\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m converter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39mconverters:\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter\u001b[38;5;241m.\u001b[39mcan_convert(arg):\n\u001b[1;32m-> 1264\u001b[0m         temp_arg \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mconvert(arg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client)\n\u001b[0;32m   1265\u001b[0m         temp_args\u001b[38;5;241m.\u001b[39mappend(temp_arg)\n\u001b[0;32m   1266\u001b[0m         new_args\u001b[38;5;241m.\u001b[39mappend(temp_arg)\n",
      "File \u001b[1;32mc:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_collections.py:510\u001b[0m, in \u001b[0;36mListConverter.convert\u001b[1;34m(self, object, gateway_client)\u001b[0m\n\u001b[0;32m    508\u001b[0m ArrayList \u001b[38;5;241m=\u001b[39m JavaClass(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava.util.ArrayList\u001b[39m\u001b[38;5;124m\"\u001b[39m, gateway_client)\n\u001b[0;32m    509\u001b[0m java_list \u001b[38;5;241m=\u001b[39m ArrayList()\n\u001b[1;32m--> 510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    511\u001b[0m     java_list\u001b[38;5;241m.\u001b[39madd(element)\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_list\n",
      "File \u001b[1;32mc:\\spark\\python\\pyspark\\sql\\column.py:718\u001b[0m, in \u001b[0;36mColumn.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 718\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    719\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_ITERABLE\u001b[39m\u001b[38;5;124m\"\u001b[39m, message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjectName\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    720\u001b[0m     )\n",
      "\u001b[1;31mPySparkTypeError\u001b[0m: [NOT_ITERABLE] Column is not iterable."
     ]
    }
   ],
   "source": [
    "df1.withColumnRenamed(\"dob\",\"Date of Birth\").show()\n",
    "df1.withColumnRenamed(df1.dob,\"Date of Birth\").show() # this is not possible because func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd2aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
