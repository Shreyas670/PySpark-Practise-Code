{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d7b208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName(\"DD\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746d0bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "776853b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|count(department)|\n",
      "+-----------------+\n",
      "|                6|\n",
      "+-----------------+\n",
      "\n",
      "+--------------------------+\n",
      "|count(DISTINCT department)|\n",
      "+--------------------------+\n",
      "|                         2|\n",
      "+--------------------------+\n",
      "\n",
      "+-------------+------------+-----------+-----------+\n",
      "|first(salary)|last(salary)|min(salary)|max(salary)|\n",
      "+-------------+------------+-----------+-----------+\n",
      "|         3000|        3300|       3000|       4600|\n",
      "+-------------+------------+-----------+-----------+\n",
      "\n",
      "+------+-----------+--------------------+\n",
      "|salary|avg(salary)|sum(DISTINCT salary)|\n",
      "+------+-----------+--------------------+\n",
      "| 21000|     3500.0|               15000|\n",
      "+------+-----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "#lets consider aggregate functions over the entire data\n",
    "\n",
    "# count()\n",
    "#If we want to get total number of rows in whole dataframe we use count() method of the dataframe\n",
    "# if we want to get count of values present in a certain record we use count() method of functions class.\n",
    "\n",
    "df.count() # this is not a transformation but an action\n",
    "\n",
    "df.select(count(\"department\")).show() #this is a transformation and it won't consider NULLs while counting like SQL\n",
    "\n",
    "#now lets look at other agg functions\n",
    "df.select(countDistinct(\"department\")).show() # this will count the distinct values in a column\n",
    "\n",
    "# Now see how to let first, last, min and max from a column\n",
    "df.select(first(\"salary\"),last(\"salary\"),min(\"salary\"),max(\"salary\")).show()\n",
    "\n",
    "#lets perform sum, avg and sumDistinct\n",
    "df.select(sum(\"salary\").alias(\"salary\"),avg(\"salary\"),sumDistinct(\"salary\")).show()  # we can use alias to give different name in output\n",
    "\n",
    "# we can also perform variance, co variance, standard deviation and much more operations\n",
    "\n",
    "#countDistinct can be also done as\n",
    "df.select(\"department\").distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77326a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|approx_count_distinct(salary)|\n",
      "+-----------------------------+\n",
      "|                            4|\n",
      "+-----------------------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|approx_count_distinct(salary)|\n",
      "+-----------------------------+\n",
      "|                            4|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in general countDistinct takes a very long time on large datasets, so when the data is very huge and getting perfect count value is not necessary we can use approx_count_distinct\n",
    "#this function will increase the performance by huge amount\n",
    "\n",
    "#it takes two parameter, one is col on which it performs count distinct and other is maximum estimation error allowed which is a float value\n",
    "\n",
    "df.select(approx_count_distinct(\"salary\",0.1)).show()\n",
    "df.agg(approx_count_distinct(\"salary\",0.1)).show()\n",
    "\n",
    "#spark also provides agg on dataframe, where aggregation is performed on whole data.\n",
    "#using agg is same as select\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ffa0bb",
   "metadata": {},
   "source": [
    "# groupBy\n",
    "Thus far, we have performed only DataFrame-level aggregations. A more common task is to perform calculations based on groups in the data. This is typically done on categorical data for which we group our data on one column and perform some calculations on the other columns that end up in that group.\n",
    "#We do this grouping in two phases. First we specify the column(s) on which we would like to group, and then we specify the aggregation(s). The first step returns a RelationalGroupedDataset, and the second step returns a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3012aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GroupedData[grouping expressions: [salary], value: [employee_name: string, department: string ... 1 more field], type: GroupBy]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[department: string, sum(Salary): bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      14700|\n",
      "|   Finance|       6300|\n",
      "+----------+-----------+\n",
      "\n",
      "+----------+-----------+\n",
      "|department|max(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|       4600|\n",
      "|   Finance|       3300|\n",
      "+----------+-----------+\n",
      "\n",
      "+----------+-----+-------+-------+\n",
      "|department|  Sum|Average|minimum|\n",
      "+----------+-----+-------+-------+\n",
      "|     Sales|14700| 3675.0|   3000|\n",
      "|   Finance| 6300| 3150.0|   3000|\n",
      "+----------+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display(df.groupBy(\"salary\")) # we can see that we get GroupedData\n",
    "display(df.groupBy(\"department\").sum(\"Salary\")) # we can see that now it is dataframe. \n",
    "\n",
    "\n",
    "df.groupBy(\"department\").sum(\"salary\").show()\n",
    "df.groupBy(\"department\").max(\"salary\").show() \n",
    "#similarly in the place of sum ,we can use max, min, avg etc.\n",
    "#instead of calculating aggregation on group one at a time, we can use agg() to calculate multiple aggregate functions in single groupBY\n",
    "\n",
    "df.groupBy(\"department\").agg(sum(\"salary\").alias(\"Sum\"),avg(\"salary\").alias(\"Average\"),min(\"salary\").alias(\"minimum\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596529d",
   "metadata": {},
   "source": [
    "# window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06269600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        James|     Sales|  3000|\n",
      "|      Michael|     Sales|  4600|\n",
      "|       Robert|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "|        James|     Sales|  3000|\n",
      "|        Scott|   Finance|  3300|\n",
      "+-------------+----------+------+\n",
      "\n",
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         2|\n",
      "|       Robert|     Sales|  4100|         3|\n",
      "|      Michael|     Sales|  4600|         4|\n",
      "+-------------+----------+------+----------+\n",
      "\n",
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   4|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number,rank,lead,lag\n",
    "\n",
    "#first we will declare the window and then use this inside over function\n",
    "window_spec= Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\",row_number().over(window_spec)).show()\n",
    "df.withColumn(\"rank\",rank().over(window_spec)).show()\n",
    "#similarly we can use dense_rank, percent_Rank, ntile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4f59ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|last|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|3300|\n",
      "|        Scott|   Finance|  3300|NULL|\n",
      "|        James|     Sales|  3000|3000|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|       Robert|     Sales|  4100|4600|\n",
      "|      Michael|     Sales|  4600|NULL|\n",
      "+-------------+----------+------+----+\n",
      "\n",
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| lag|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|NULL|\n",
      "|        Scott|   Finance|  3300|3000|\n",
      "|        James|     Sales|  3000|NULL|\n",
      "|        James|     Sales|  3000|3000|\n",
      "|       Robert|     Sales|  4100|3000|\n",
      "|      Michael|     Sales|  4600|4100|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now lets look at value window functions such as lead,lag\n",
    "window_spec2= Window.partitionBy(df.department).orderBy(df.salary)\n",
    "\n",
    "df.withColumn(\"last\",lead(df.salary,1).over(window_spec2)).show()\n",
    "df.withColumn(\"lag\",lag(df.salary,1).over(window_spec2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f30ef547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| max|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|3000|\n",
      "|        Scott|   Finance|  3300|3300|\n",
      "|        James|     Sales|  3000|3000|\n",
      "|        James|     Sales|  3000|3000|\n",
      "|       Robert|     Sales|  4100|4100|\n",
      "|      Michael|     Sales|  4600|4600|\n",
      "+-------------+----------+------+----+\n",
      "\n",
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| max|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|3300|\n",
      "|        Scott|   Finance|  3300|3300|\n",
      "|        James|     Sales|  3000|4600|\n",
      "|        James|     Sales|  3000|4600|\n",
      "|       Robert|     Sales|  4100|4600|\n",
      "|      Michael|     Sales|  4600|4600|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now lets see window functions for aggregate functions\n",
    "from pyspark.sql.functions import max\n",
    "df.withColumn(\"max\",max(df.salary).over(window_spec2)).show()\n",
    "\n",
    "# we see that max is not working properly this is because current frame size is unbounded preceeding to current row\n",
    "#however we can change the frame size using rowsBetween\n",
    "\n",
    "window_spec3= Window.partitionBy(df.department).orderBy(df.salary).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "df.withColumn(\"max\",max(df.salary).over(window_spec3)).show()\n",
    "#now we see its working porperly\n",
    "\n",
    "#similarly we can use other aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de1fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
