{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b468564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('caching2').master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e42fde40",
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = 'Year Double, Flood Double'\n",
    "df = spark.read.format('csv').schema(_schema).option('Header','true').load('nile.csv')\n",
    "\n",
    "# If we don't provide a schema although read is not an action, spark will trigger a job to infer the schema from the data.\n",
    "# Hence we provide schema here so that no job is trigerred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e27152cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "| Year|  Flood|\n",
      "+-----+-------+\n",
      "|101.0| 11.037|\n",
      "|102.0|10.9986|\n",
      "|103.0| 11.037|\n",
      "|104.0| 10.151|\n",
      "|105.0|11.1906|\n",
      "|106.0|10.7284|\n",
      "|107.0|10.9218|\n",
      "|108.0|  10.69|\n",
      "|109.0|10.5748|\n",
      "|110.0|10.9986|\n",
      "|111.0| 10.941|\n",
      "|112.0|10.4596|\n",
      "|113.0|10.9602|\n",
      "|114.0| 10.151|\n",
      "|115.0|10.0742|\n",
      "|116.0|10.1126|\n",
      "|117.0|  9.767|\n",
      "|118.0|10.4212|\n",
      "|119.0|10.1894|\n",
      "|120.0|10.4212|\n",
      "+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where('Year>100').show()\n",
    "\n",
    "# we can see from UI that first job is trigerred here, data is read from csv and then applied filter on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "710ecfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "570"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache().count()\n",
    "#we have to run a action that performs action on whole dataframe for it be cached. If we run show() action, as it won't read whole dataframe, the dataframe will not be cached\n",
    "\n",
    "# in storage menu we can see that this dataframe is cached to the memory. Cache only happens after an action is triggered as it is also lazily evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cff72225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "| Year|  Flood|\n",
      "+-----+-------+\n",
      "|101.0| 11.037|\n",
      "|102.0|10.9986|\n",
      "|103.0| 11.037|\n",
      "|104.0| 10.151|\n",
      "|105.0|11.1906|\n",
      "|106.0|10.7284|\n",
      "|107.0|10.9218|\n",
      "|108.0|  10.69|\n",
      "|109.0|10.5748|\n",
      "|110.0|10.9986|\n",
      "|111.0| 10.941|\n",
      "|112.0|10.4596|\n",
      "|113.0|10.9602|\n",
      "|114.0| 10.151|\n",
      "|115.0|10.0742|\n",
      "|116.0|10.1126|\n",
      "|117.0|  9.767|\n",
      "|118.0|10.4212|\n",
      "|119.0|10.1894|\n",
      "|120.0|10.4212|\n",
      "+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where('Year>100').show()\n",
    "#the same filter applied before will run faster now and will read data from cache rather than performing csv scan.\n",
    "# we can also check DAG in UI for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d39abc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: double, Flood: double]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to remove the data from cache we use unpersist\n",
    "df.unpersist()\n",
    "\n",
    "#now if we see storage, there will be nothing and any tranformation on df will be read from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba33a719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "470"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caching will remember the lineage to figure out if data should be read from cache or source.\n",
    "# lets cache a part of df\n",
    "\n",
    "df.where('Year>100').cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35e7a44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets try to get a subset of above cached data.\n",
    "\n",
    "df.where('Year > 150').count()\n",
    "\n",
    "# we can see that although given by the above filter was present in the cached dataframe itself, spark will read the csv.\n",
    "# this is because lineage is not same for the above cached dataframe and this output dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c73a114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to clear entire cache, we can use below\n",
    "\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50e60337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets look at persist\n",
    "import pyspark\n",
    "df_persist = df.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "#lets call an action to perform the persist\n",
    "\n",
    "df_persist.write.format('noop').mode('overwrite').save()\n",
    "\n",
    "#if we want to perform a dummy write action without actually writing the data anywhere, we can use noop as the format\n",
    "\n",
    "# Memory Serialized 1x Replicated \n",
    "# we can see that data is now serialized hence takes less space, but this takes time compared to deserialized.\n",
    "# cache will stored deserialized, however persist will store serialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3057a514",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'StorageLevel' has no attribute 'MEMORY_ONLY_SER'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_persist_ser \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mpersist(pyspark\u001b[38;5;241m.\u001b[39mStorageLevel\u001b[38;5;241m.\u001b[39mMEMORY_ONLY_SER)\n\u001b[0;32m      2\u001b[0m df_persist_ser\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoop\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msave()\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'StorageLevel' has no attribute 'MEMORY_ONLY_SER'"
     ]
    }
   ],
   "source": [
    "df_persist_ser = df.persist(pyspark.StorageLevel.MEMORY_ONLY_SER)\n",
    "df_persist_ser.write.format('noop').mode('overwrite').save()\n",
    "\n",
    "# as we know in pyspark, persist by default serializes the data, hence SER options are not applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab83c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()\n",
    "df_persist_disk= df.persist(pyspark.StorageLevel.DISK_ONLY)\n",
    "df_persist_disk.write.format('noop').mode('overwrite').save()\n",
    "\n",
    "#Disk Serialized 1x Replicated, and we can see data is stored on disk only not on memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cb045e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets try the replication\n",
    "spark.catalog.clearCache()\n",
    "df_persist_2= df.persist(pyspark.StorageLevel.MEMORY_ONLY_2)\n",
    "df_persist_2.write.format('noop').mode('overwrite').save()\n",
    "\n",
    "# Memory Serialized 2x Replicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "315d6680",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "098615b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o137.checkpoint.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.rdd.ReliableCheckpointRDD.getPartitions(ReliableCheckpointRDD.scala:74)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\r\n\tat org.apache.spark.rdd.ReliableCheckpointRDD$.writeRDDToCheckpointDirectory(ReliableCheckpointRDD.scala:179)\r\n\tat org.apache.spark.rdd.ReliableRDDCheckpointData.doCheckpoint(ReliableRDDCheckpointData.scala:60)\r\n\tat org.apache.spark.rdd.RDDCheckpointData.checkpoint(RDDCheckpointData.scala:75)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$doCheckpoint$1(RDD.scala:1927)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDD.doCheckpoint(RDD.scala:1917)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$checkpoint$1(Dataset.scala:736)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:727)\r\n\tat org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:690)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetCheckpointDir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAWS Udemy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m df121\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mcheckpoint()\n",
      "File \u001b[1;32mc:\\spark\\python\\pyspark\\sql\\dataframe.py:1032\u001b[0m, in \u001b[0;36mDataFrame.checkpoint\u001b[1;34m(self, eager)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheckpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, eager: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a checkpointed version of this :class:`DataFrame`. Checkpointing can be used to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;124;03m    truncate the logical plan of this :class:`DataFrame`, which is especially useful in\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;124;03m    iterative algorithms where the plan may grow exponentially. It will be saved to files\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;124;03m    DataFrame[age: bigint, name: string]\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1032\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcheckpoint(eager)\n\u001b[0;32m   1033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mc:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o137.checkpoint.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.rdd.ReliableCheckpointRDD.getPartitions(ReliableCheckpointRDD.scala:74)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\r\n\tat org.apache.spark.rdd.ReliableCheckpointRDD$.writeRDDToCheckpointDirectory(ReliableCheckpointRDD.scala:179)\r\n\tat org.apache.spark.rdd.ReliableRDDCheckpointData.doCheckpoint(ReliableRDDCheckpointData.scala:60)\r\n\tat org.apache.spark.rdd.RDDCheckpointData.checkpoint(RDDCheckpointData.scala:75)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$doCheckpoint$1(RDD.scala:1927)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDD.doCheckpoint(RDD.scala:1917)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$checkpoint$1(Dataset.scala:736)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:727)\r\n\tat org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:690)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.setCheckpointDir('D:\\AWS Udemy')\n",
    "df121=df.checkpoint()\n",
    "\n",
    "# Although we are getting error, we can see that an ORC file is  created at the above location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23cd08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
