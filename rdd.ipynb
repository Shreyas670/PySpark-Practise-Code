{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80c6b2dc-fe46-42a2-bf37-62c1a4c07be4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# We need to import sparkconf which will set the configurations related to sparkcontext and then use it to create sparkcontext\n",
    "# note that here we are importing those methods from pyspark module not pyspark.sql module unlike sparksession\n",
    "\n",
    "conf=SparkConf().setAppName(\"Exam\").setMaster(\"local\")\n",
    "spark= SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6716233-b906-4f55-ad9d-f51d0dcd91b8",
   "metadata": {},
   "source": [
    "# Creating RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22fc6c1e-ace0-4026-bce4-c08654a11366",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Shreyas', 24]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd= spark.parallelize([\"Shreyas\",24,\"Jayaram\", 60])\n",
    "display(myrdd)\n",
    "\n",
    "myrdd.take(2)\n",
    "#using take we can retrive the number of objects we want from an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f50df8-eaf5-4e83-a03d-599b5c164a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Spark, this is a sample file.',\n",
       " 'This file is for practicing Spark RDD operations.',\n",
       " 'Spark is powerful and Spark is fast.',\n",
       " 'We will count all the words in this file.',\n",
       " 'This is a simple example.',\n",
       " 'A simple, simple example.',\n",
       " 'Hello again, Spark.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating RDD from external files\n",
    "#we use textFile method of sparkcontext, this create each line in the text file as a record in RDD.\n",
    "#we can also use this method to import csv and other files, which we will see later.\n",
    "\n",
    "rdd1=spark.textFile('sample.txt')\n",
    "rdd1.collect()\n",
    "\n",
    "#collect action returns all the data in rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8243347-de4b-4172-8dc3-574acd9c4d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/home/jupyter/data/sample.txt',\n",
       "  'Hello Spark, this is a sample file.\\nThis file is for practicing Spark RDD operations.\\nSpark is powerful and Spark is fast.\\nWe will count all the words in this file.\\nThis is a simple example.\\nA simple, simple example.\\nHello again, Spark.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also create RDD which considers all the content in a file as one record using wholeTextFiles method\n",
    "\n",
    "rdd2=spark.wholeTextFiles('sample.txt')\n",
    "rdd2.collect()\n",
    "\n",
    "#here everytime two records will be created, one is name of the file and second is all the content inside the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b3e48-98a8-484a-a6b8-c7b6ebe1ec47",
   "metadata": {},
   "source": [
    "# Actions on RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce15503-47a4-4978-88d3-ac7dd8ed484e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shreyas'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets look at some basic actions on rdd\n",
    "\n",
    "myrdd.take(3) # gives first 3 records\n",
    "myrdd.collect() # gives all records\n",
    "myrdd.count() # gives count of records in rdd\n",
    "# myrdd.min(), myrdd.max() # used to return the min, max elements in RDD\n",
    "#however in this example where rdd contains both integer and string, min/max will throw error\n",
    "myrdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f94a9ad3-fc7b-40be-8664-6c23ffc28e72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduce - this action will “reduce” an RDD of any kind of value to one value.\n",
    "RDD1 = spark.parallelize([1,3,2,4])\n",
    "add = lambda x,y: x + y\n",
    "RDD1.reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94675034-d02e-4d77-9fcf-b8c10a809213",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saveAsTextFile - this will save the rdd as text file.with each record in different lines.\n",
    "#myrdd.saveAsTextFile('sample.txt')\n",
    "\n",
    "#getNumPartitions() — returns the number of Partitions\n",
    "myrdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac8877-3aa3-4e12-a2d3-5cfd12a676ce",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3e872c6-511d-4f17-99da-97ed1c916c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map() — transformation takes in an anonymous function and applies this function to each of the elements in the RDD\n",
    "# As this function applies to each element in the rdd, the input parameter for the lambda function should be only one record and output should also be one record.\n",
    "# If we create a lambda function like lambda x,y: x+y it will throw an error\n",
    "myrdd = spark.parallelize([1,2,3,4,5])\n",
    "myrdd.map(lambda x:x+1).collect()\n",
    "\n",
    "# or\n",
    "\n",
    "def addone(x):\n",
    "    return x+1\n",
    "myrdd.map(addone).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4676b05-4744-41a7-b737-ab548ff263c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi', 'this'], ['Hi', 'is', 'shreyas']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatmap - it is a tranformation that flattens the RDD after applying the function to each element\n",
    "#To flatten means to reduce the dimensionality. In simpler terms, it means reducing a multidimensional to specific dimension.\n",
    "#lets look at working of map on below example\n",
    "\n",
    "rdd1= spark.parallelize([\"Hi this\",\"Hi is shreyas\"])\n",
    "rdd1.map(lambda x: x.split(\" \")).collect()\n",
    "\n",
    "#we can see that map split both of them and gave 2 dimensional rdd, which has two words in each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ffc3260-ce60-47ce-84de-afbb090e601f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'this', 'Hi', 'is', 'shreyas']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd1.flatMap(lambda x: x.split(\" \"))\n",
    "rdd2.collect()\n",
    "#however we can see that this function has flatten the rdd, that is converted 2d to 1d and returned 4 words.\n",
    "# flatmap also takes one input parameter, but the return value of the lambda should be an iterable such as an array. If its not an iterable spark will throw an error.\n",
    "# flatmap will flatten the iterable. In the above example split returns an array, which is an iterable and it is exploded/flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6b5ff6-b10c-47eb-a2e9-170d38b36e7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# reduce transformations\n",
    "till now we learnt about the flatMap and map transformations, now lets look at reduce and group operations.\n",
    "We can't run the reduce and group operations on the simple rdd we created above. We will need pair rdd, that is key-value rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97ffb558-3e41-46a9-8abc-246393c95d79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'this', 'Hi', 'is', 'shreyas']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "961ff064-390f-4c5c-9449-1be5b56d4006",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 1), ('this', 1), ('Hi', 1), ('is', 1), ('shreyas', 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets create a pair rdd using the rdd1 we have above\n",
    "\n",
    "rdd_pair =  rdd2.map(lambda x:(x,1))\n",
    "rdd_pair.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f37e25fe-3225-4fac-8a98-fdd93c14f0d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'this', 'Hi', 'is', 'shreyas']\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "# to get all keys and values from a rdd\n",
    "\n",
    "print(rdd_pair.keys().collect())\n",
    "print(rdd_pair.values().collect())\n",
    "\n",
    "#to get value associated with a particular key\n",
    "print(rdd_pair.lookup('Hi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a81ade4-1721-488a-9dc7-ada420f75b90",
   "metadata": {},
   "source": [
    "# groupByKey() vs reduceByKey(fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da343377-26c4-48da-be00-679ac49d28fd",
   "metadata": {},
   "source": [
    "In PySpark, groupByKey groups values into an iterable collection for each key without aggregation, while reduceByKey groups values and applies a reduction function to combine them into a single value per key. reduceByKey is more efficient for aggregation tasks as it performs local aggregations (combiner logic) before shuffling data, significantly reducing the amount of data transferred over the network. groupByKey performs a full shuffle of all values, which can be less performant and memory-intensive for large datasets, and is only suitable when the full list of values is required for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90ce4fb7-dc6e-4b0a-beaf-924ac467e975",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi', <pyspark.resultiterable.ResultIterable object at 0x772b4dd835d0>), ('this', <pyspark.resultiterable.ResultIterable object at 0x772b4dd839d0>), ('is', <pyspark.resultiterable.ResultIterable object at 0x772b4dd83990>), ('shreyas', <pyspark.resultiterable.ResultIterable object at 0x772b4dd83a10>)]\n"
     ]
    }
   ],
   "source": [
    "# groupByKey()\n",
    "#It takes key-value pairs (K, V) as an input, groups the values based on the key(K), and generates a dataset of KeyValueGroupedDataset(K, Iterable) pairs as an output. \n",
    "\n",
    "rdd5= rdd_pair.groupByKey()\n",
    "\n",
    "print(rdd5.collect())\n",
    "\n",
    "# answer is like (key, iterable(val1, val2,,,,valn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "426b263d-f1c1-48b7-a42a-7f7accfd9ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 2), ('this', 1), ('is', 1), ('shreyas', 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can apply functions on the iterable and calculate the sum\n",
    "rdd5.mapValues(len).collect()\n",
    "\n",
    "# here len is a function which calculate the length of the iterable doing which we will get the count of occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f81f1de-76b2-450a-9a4d-8d8118f599c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 2), ('this', 1), ('is', 1), ('shreyas', 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets see we can do this using reduceByKey()\n",
    "\n",
    "rdd6 = rdd_pair.reduceByKey(lambda x,y: x+y)\n",
    "rdd6.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae2ad4f-9ab6-4eaa-8be2-d09f86c97659",
   "metadata": {},
   "source": [
    "Using reduceByKey() spark will first aggregate the data in the source partitions iself then performs shuffle and then again calculates the aggregation. Thus the amout of data getting shuffled is less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "935e6a15-7533-4f1c-921c-63e61444d561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
