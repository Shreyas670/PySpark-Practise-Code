{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b98b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName(\"ssql\").master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5bff0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|Shreyash| 30|\n",
      "|Rakshith| 25|\n",
      "|    John| 10|\n",
      "+--------+---+\n",
      "\n",
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|John| 10|\n",
      "+----+---+\n",
      "\n",
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|John| 10|\n",
      "+----+---+\n",
      "\n",
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|John| 10|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To filter data and get desired rows from a dataframe, we use filter() or whe re() functions\n",
    "#Filter can be used on one condition or multiple conditions, seperated by an d/or.\n",
    "from pyspark.sql.functions import *\n",
    "data=[(\"Shreyash\", 30), (\"Rakshith\", 25), (\"John\",10)]\n",
    "schema=[\"name\",\"age\"]\n",
    "df=spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "df.filter(col(\"age\") <20).show()\n",
    "df.filter(df.age<20).show()\n",
    "df.where(df.age<20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ff5ae8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|Rakshith| 25|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|Shreyash| 30|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|Shreyash| 30|\n",
      "|Rakshith| 25|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|Shreyash| 30|\n",
      "|Rakshith| 25|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also give multiple filter conditions\n",
    "df.filter((df.age>20) & (df.age<28)).show() #note that each condition should b e inside brackets.\n",
    "#now Lets see how other filter conditions such as startswith, endswith, isnul L, etc. works\n",
    "df.filter(df.name.startswith(\"S\")).show()\n",
    "df.filter(df.name.endswith(\"h\")).show()\n",
    "df.filter(df.name.contains(\"sh\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f8f1ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|Shreyash| 30|\n",
      "|    John| 10|\n",
      "+--------+---+\n",
      "\n",
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "+----+---+\n",
      "\n",
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|John| 10|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.age.isin (10,30)).show()\n",
    "df.filter(df.age.isNull()).show() #similarly use isNotNull()\n",
    "df.filter(df.name.like(\"%oh%\")).show() #this is similar to Like in sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d43902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|idd|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  2|  3|\n",
      "|  1|  3|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| id|idd|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  1|  3|\n",
      "|  2|  3|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| id|idd|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  1|  3|\n",
      "|  2|  3|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| id|idd|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  2|  3|\n",
      "+---+---+\n",
      "\n",
      "+---+\n",
      "|idd|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "|idd|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now Lets see how we can get unique records using distinct function and dropD upliates function.\n",
    "# distinct will consider all the column, however with dropDupplicates, we can provide column names as parameter to apply distinct on only those columns\n",
    "data1=[(1,1), (1,1), (2,2), (2,3), (1,3)]\n",
    "schema1=[\"id\", \"idd\"]\n",
    "df1=spark.createDataFrame(data1, schema1)\n",
    "df1.show()\n",
    "df1.distinct().show()\n",
    "df1.dropDuplicates().show()\n",
    "df1.dropDuplicates ([\"idd\"]).show() # we can also give multiple columns inside dropduplicates. Columns should be given inside a list. We can also calculate c ount after getting distinct using count function instead of show.\n",
    "#We can also use distinct to get unique records in a particular column Like b eLow\n",
    "df1.select(df1.idd). distinct().show() #this will select only particular column s first then apply distinct, hence we are not getting all columns in output. T his is similarly done with dropDuplicates like below\n",
    "df1.dropDuplicates ([\"idd\"]).select(\"idd\").show() #here duplicates are removed from idd, and then we select to view only idd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bc3303c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|Shreyash| 30|\n",
      "|Rakshith| 25|\n",
      "|    John| 10|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|    John| 10|\n",
      "|Rakshith| 25|\n",
      "|Shreyash| 30|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|    John| 10|\n",
      "|Rakshith| 25|\n",
      "|Shreyash| 30|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|Shreyash| 30|\n",
      "|Rakshith| 25|\n",
      "|    John| 10|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|Shreyash| 30|\n",
      "|Rakshith| 25|\n",
      "|    John| 10|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|    John| 10|\n",
      "|Rakshith| 25|\n",
      "|Shreyash| 30|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To sort the data in dataframe, You can use either sort() or orderBy() functi on of PySpark DataFrame to sort DataFrame by ascending or descending order bas ed on single or multiple columns. Both methods take one or more columns as arg uments and return a new DataFrame after sorting.\n",
    "df.show()\n",
    "df.sort(\"age\").show()\n",
    "df.sort(df.name).show()\n",
    "df.sort(df.name.desc()).show() # we can specify desc or asc in multiple ways, by default it will be asc.\n",
    "df.orderBy(desc(\"age\")).show()\n",
    "df.orderBy([\"age\", \"name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f690c118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
