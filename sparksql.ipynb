{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a60833ca-5244-4e97-89dc-88c2e5e01272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"sparksql\").master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384567fa-3e2c-402b-b09f-2778727e0508",
   "metadata": {},
   "source": [
    "# Spark Catalog: \n",
    "\n",
    "The Spark Catalog is a component within Spark SQL that acts as a central repository for metadata about structured data. Think of it as a system that organizes and manages information about your databases, tables, views, functions, and other data assets within your Spark environment. \n",
    "\n",
    "Spark supports different catalog implementations: \n",
    "\n",
    "in-memory: A simple, ephemeral catalog that exists only for the current SparkSession. This is the default if Spark is not built with Hive support. \n",
    "\n",
    "hive: Uses the Hive Metastore to persist metadata. This allows for sharing metadata across different Spark sessions and with Hive. This is the default if Spark is built with Hive support. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0823fc7-ec13-4a59-9ac4-661054541fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in-memory'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.catalogImplementation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad63ab15-57d3-4c6a-b705-ef130999a2be",
   "metadata": {},
   "source": [
    "# Spark warehouse directory \n",
    "\n",
    "The Spark warehouse directory is the default location on the file system where Spark SQL stores the data and metadata for managed tables (also known as internal tables). When you create a managed table in Spark SQL without specifying a LOCATION, Spark will store the table's data files within this directory.\n",
    "Currently I am implementing delta lake using spark and hence warehouse.dir is set to a s3 location. Else the default is current working directory where spark application is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcde678f-46bd-405a-8f62-70065aa0d186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://datalakeshreyas/pyspark_practise/warehouse'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.warehouse.dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e34c5-d69e-4720-845c-1deba343fe33",
   "metadata": {},
   "source": [
    "# Tables\n",
    "There are two types:\n",
    "1. Managed Tables: Spark manages both the metadata and the data for these tables. The data is typically stored in the Spark warehouse directory. If you drop a managed table, both the metadata and the underlying data are deleted.\n",
    "2. External Tables: Spark only manages the metadata, and the data resides in a location you specify. If you drop an external table, only the metadata is removed, and the data remains untouched in the specified LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf397089-a4aa-4ca1-8079-6469398176f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE [IF NOT EXISTS] table_name\n",
    "  (column1 data_type [COMMENT column_comment],\n",
    "   column2 data_type [COMMENT column_comment],\n",
    "   ...)\n",
    "[USING format]\n",
    "[OPTIONS (key=value, ...)]\n",
    "[PARTITIONED BY (column_name, ...)]\n",
    "[CLUSTERED BY (column_name, ...) [SORTED BY (column_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]\n",
    "[LOCATION 'path']\n",
    "[COMMENT 'table_comment']\n",
    "[TBLPROPERTIES (key=value, ...)]\n",
    "[AS select_statement];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf0d4a-d886-4dce-9333-99d0d72ce60e",
   "metadata": {},
   "source": [
    "The syntax is same for External tables. Only difference is we use CREATE EXTERNAL TABLE at the starting.\n",
    "\n",
    "[USING format] (Optional for managed tables, often used for external tables): Specifies the data source format for the table. Common formats include:\n",
    "1. parquet (default in many Spark configurations)\n",
    "2. csv\n",
    "3. json\n",
    "4. orc\n",
    "5. avro\n",
    "6. text\n",
    "7. delta (for Delta Lake tables)\n",
    "8. jdbc (for connecting to relational databases)\n",
    "9. hive (if Spark is built with Hive support)\n",
    "\n",
    "If omitted for managed tables, the default format configured in Spark is used (usually Parquet). For external tables, it's good practice to specify the format.   \n",
    "\n",
    "[OPTIONS] : Used to provide additional options based on the format. Like header = 1 for csv format or jdbc info when jdbc format is selected.\n",
    "\n",
    "[PARTITIONED BY (column_name, ...)]: Creates a partitioned table. Data is divided into directories based on the values of the specified columns. This can significantly improve query performance for queries that filter on these columns.\n",
    "\n",
    "[LOCATION 'path']:\n",
    "For managed tables: This is optional. If specified, Spark will store the data for the table in the given path within the Spark warehouse. If omitted, Spark will choose a default location within the warehouse.\n",
    "\n",
    "For external tables: This is mandatory. It specifies the directory where the data files for the table are located. The path should be accessible by the Spark cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06894ad3-163a-4096-b385-e0d36a72d351",
   "metadata": {},
   "source": [
    "# Views\n",
    "\n",
    "Spark SQL supports two main types of views:\n",
    "\n",
    "Temporary Views (Session-Scoped):\n",
    "\n",
    "1. These views exist only for the duration of the current SparkSession.\n",
    "2. They are not persisted in the underlying catalog (like the Hive Metastore).\n",
    "3. Once the SparkSession ends, the temporary view is automatically dropped.\n",
    "4. They are useful for intermediate results within a specific analysis or application.\n",
    "5. They are created using the CREATE TEMPORARY VIEW or CREATE OR REPLACE TEMPORARY VIEW statement.\n",
    "\n",
    "\n",
    "Global Temporary Views (Application-Scoped):\n",
    "\n",
    "1. These views are also temporary but are visible across all SparkSessions within the same Spark application.\n",
    "2. They are stored in a system database called global_temp.\n",
    "3. To access a global temporary view, you need to qualify its name with the global_temp database (e.g., SELECT * FROM global_temp.my_global_view).\n",
    "4. They are useful when you want to share intermediate results or common query logic across multiple parts of the same Spark application.\n",
    "5. They are created using the CREATE GLOBAL TEMPORARY VIEW or CREATE OR REPLACE GLOBAL TEMPORARY VIEW statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da94569-4a5e-4d97-8636-3e668fd86f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also create temporary and global views in pyspark on a dataframe\n",
    "df.createOrReplaceTempView(\"people_temp_view\")\n",
    "\n",
    "# Now you can query this temporary view using spark.sql()\n",
    "spark.sql(\"SELECT * FROM people_temp_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8896d87-c5da-448f-9777-8d3c05b3e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceGlobalTempView(\"people_global_temp_view\")\n",
    "\n",
    "# You can access this global temporary view using spark.sql() with the global_temp prefix\n",
    "spark.sql(\"SELECT * FROM global_temp.people_global_temp_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ebd814-c341-4e73-a2ef-02cc175de546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a660c4aa-a621-4363-8489-41bd498320b3",
   "metadata": {},
   "source": [
    "# SparkSQl, Hive and Snowflake\n",
    "Most of the spark sql and hive is similar to snowflake. We have external tables there as well and it is same here, we can create external tables, give partitions as well, we cannot perform DML on them, if we want to bring in new data then we perform alter table refresh similar to snowflake.\n",
    "\n",
    "Even Syntax like USE DATABASE is same. We have also CREATE IF NOT EXISTS, CREATE OR REPLACE, also all the file formats and its options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd66af-b2bf-4b42-ab0e-915f22c66993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
