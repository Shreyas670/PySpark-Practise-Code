{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e3e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('joins').master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71ab5646",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.range(10)\n",
    "df2= spark.range(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792f5c6a",
   "metadata": {},
   "source": [
    "# Broadcast Hash Join\n",
    "In this join strategy, when one of the dataframe to be joined is smaller, then instead of shuffling we will broadcast the smaller dataframe to all the worker nodes and it will be joined internally with the other dataframe inside workers.\n",
    "\n",
    "There is a threshold which specifies the maximum size of the dataframe that can be broadcasted, it is 10MB by default but we can update it to maximum  of 8GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9794a5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10485760b'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see that default is set to 10MB\n",
    "spark.conf.get('spark.sql.autoBroadcastJoinThreshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8df2debc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [id#179L], [id#181L], Inner, BuildLeft, false\n",
      "   :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=594]\n",
      "   :  +- Range (0, 10, step=1, splits=1)\n",
      "   +- Range (0, 10000, step=1, splits=1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.id==df2.id,'inner').explain()\n",
    "\n",
    "#we can see that spark automatically performing broadcast join as the size of df1 is less than threshold of 10MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac948957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [id#179L], [id#181L], Inner\n",
      "   :- Range (0, 10, step=1, splits=1)\n",
      "   +- Range (0, 10000, step=1, splits=1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If we want spark not to apply broadcast join, we can do that by setting the threshold to -1\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "df1.join(df2,df1.id==df2.id,'inner').explain()\n",
    "\n",
    "#now we can see that sort merge join is happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7b04a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [id#181L], [id#179L], Inner, BuildRight, false\n",
      "   :- Range (0, 10000, step=1, splits=1)\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=621]\n",
      "      +- Range (0, 10, step=1, splits=1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# even after setting threshold to -1 to disable automatic broadcast join, we can override it by explicitly\n",
    "#by using broadcast function to broadcast the smaller dataframe\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df2.join(broadcast(df1),df1.id==df2.id,'inner').explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacaa99f",
   "metadata": {},
   "source": [
    "Only supported for equi-joins, while the join keys do not need to be sortable.\n",
    "\n",
    "Supported for all join types except full outer joins.\n",
    "\n",
    "In case of left outer and right outer joins, the small table should not be present in left incase of left join and on right in case of right join, else the small table will not be broadcasted.\n",
    "\n",
    "BHJ usually performs faster than the other join algorithms when the broadcast side is small. However, broadcasting tables is a network-intensive operation and it could cause OOM or perform badly in some cases, especially when the build/broadcast side is big.\n",
    "\n",
    "Even when the two dataframes size is less than threshold, then the smaller dataframe among the two will be broadcasted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf962261",
   "metadata": {},
   "source": [
    "# Shuffle Hash Join\n",
    "Shuffles data across the executors based on join key to gather data having same join keys into the same executor. Then performs hash join on the data inside the partition.\n",
    "\n",
    "For spark to prefer this join, we need to disable both broadcast hash join and sort merge join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "322bbc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1fc25fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [id#324L], [id#326L], Inner\n",
      "   :- Range (0, 100, step=1, splits=1)\n",
      "   +- Range (0, 90, step=1, splits=1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a= spark.range(100)\n",
    "b=spark.range(90)\n",
    "a.join(b,a.id==b.id,'inner').explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a344e5",
   "metadata": {},
   "source": [
    "Even though when we set sortmerge to be false, spark can still force it based on certain factors.\n",
    "For shuffle hashed join to be preferred, along with sortmerge join set to false, we need to set other configurations accordingly.\n",
    "Spark will prefer shuffle hash join only when hashing a certain partition is less costlier than sorting it. If our dataframes are both of nearly same sizes, then shuffle hashed join will not be prefer as sorting is much cheaper than hashing in this case.\n",
    "However, we have a parameter called shuffledHashJoinFactor which is default 3, which tells us that size of one dataframe should be 3 times more than size of other.\n",
    "\n",
    "https://stackoverflow.com/questions/57987613/spark-sortmergejoin-is-not-changing-to-shufflehashjoin\n",
    "\n",
    "Another factor is size of a partition to be hashed should be lesser than broadcastthreshold * shufflepartitions, in our case if we set broadcast threshold to -1 or 0 then it will always be false, hence shuffle hash joined will not be preferred. Hence we need to set the broadcastThreshold to atleast 2 or 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9e6bc0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see that default value is 3\n",
    "spark.conf.get(\"spark.sql.shuffledHashJoinFactor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5b0c95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ShuffledHashJoin [id#332L], [id#334L], Inner, BuildRight\n",
      "   :- Range (0, 100, step=1, splits=1)\n",
      "   +- Range (0, 9, step=1, splits=1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now lets set our configurations for shuffle hashed joined to be preferred.\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 2)\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", False)\n",
    "\n",
    "a1= spark.range(100)\n",
    "b1=spark.range(9)\n",
    "a1.join(b1,a1.id==b1.id,'inner').explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71de4ebe",
   "metadata": {},
   "source": [
    "we can see that shuffle hash join is preferred here as one dataframe is three time less size than other and also other configurations are set accordingly.\n",
    "\n",
    "The join keys need NOT be sortable\n",
    "This join supports only equi joins. Also all types of joins excluding full outer.\n",
    "This is a costly operations as it involves both shuffling and hashing and hash tables takes both memory and computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51bf999",
   "metadata": {},
   "source": [
    "# Shuffle Sort Merge Join\n",
    "Involved shuffling the data to get the same join keys in the same executors and then sort-merge join on them.\n",
    "Join keys should be sortable in this unlike the above two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac5b9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although this join is default, lets us change the configuration back to True as we had changed it to false.\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bab7d854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [Id#348L, Name#349, Age#353L]\n",
      "   +- SortMergeJoin [Id#348L], [Id#352L], Inner\n",
      "      :- Sort [Id#348L ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(Id#348L, 200), ENSURE_REQUIREMENTS, [plan_id=956]\n",
      "      :     +- Filter isnotnull(Id#348L)\n",
      "      :        +- Scan ExistingRDD[Id#348L,Name#349]\n",
      "      +- Sort [Id#352L ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(Id#352L, 200), ENSURE_REQUIREMENTS, [plan_id=957]\n",
      "            +- Filter isnotnull(Id#352L)\n",
      "               +- Scan ExistingRDD[Id#352L,Age#353L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataf = spark.createDataFrame([(1,\"Shreyas\"),(2,\"Jayaram\")],[\"Id\",\"Name\"])\n",
    "datag = spark.createDataFrame([(1,25),(2,60)],[\"Id\",\"Age\"])\n",
    "\n",
    "dataf.join(datag,['id'],'inner').explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ab24f",
   "metadata": {},
   "source": [
    "We can see that data is shuffled first and then sorted and sortmerge is applied.\n",
    "\n",
    "Works only for equijoins.\n",
    "\n",
    "Supports all types of joins including full outer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3bcd2d",
   "metadata": {},
   "source": [
    "# Cartesian Join\n",
    "Cartesian product join ( Shuffle and Replication Nested loop Join) works very similar to broadcast nested loop join except the dataframe is not brodcasted.\n",
    "\n",
    "Shuffle and replication does not mean a true shuffle as in records with same keys are sent to the same partitions.\n",
    "Instead an entire partition of the dataset is sent over or replicated to all the partitions for a full cross or nested loop join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b08646bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "CartesianProduct (ID#366L >= ID#368L)\n",
      ":- *(1) Filter isnotnull(ID#366L)\n",
      ":  +- *(1) Scan ExistingRDD[ID#366L]\n",
      "+- *(2) Filter isnotnull(ID#368L)\n",
      "   +- *(2) Scan ExistingRDD[ID#368L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c1 = spark.createDataFrame([[1],[2],[3]],\"ID Long\")\n",
    "c2 = spark.createDataFrame([[4],[2],[10],[20]],\"ID Long\")\n",
    "\n",
    "c1.join(c2, c1.ID >= c2.ID).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b19ee5",
   "metadata": {},
   "source": [
    "We can see that as we have used non equi join condition, cartesian join is getting performed as the above three joins does not work on non equi, spark will automatically choose this.\n",
    "\n",
    "Very expensive algorithm, high possibility of OOM errors.\n",
    "Works only with inner types of joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f9e68a",
   "metadata": {},
   "source": [
    "# Broadcast Nested Loop Join\n",
    "\n",
    "Broadcast Nested Loop join works by broadcasting one of the entire datasets and performing a nested loop to join the data. So essentially every record from dataset 1 is attempted to join with every record from dataset 2.\n",
    "\n",
    "As you could guess, Broadcast Nested Loop is not preferred and could be quite slow.\n",
    "\n",
    "Smallest dataset is broadcasted to all executors or tasks processing the bigger dataset Left side will be broadcasted in a right outer join. Right side in a left outer, left semi, left anti or existence join will be broadcasted. Either side can be broadcasted in an inner-like join.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fcf04fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets set broadcast threshold back to 10MB for this join to be selected by spark.\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10485760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eea112f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildLeft, Inner, (id#432L >= id#434L)\n",
      "   :- BroadcastExchange IdentityBroadcastMode, [plan_id=1140]\n",
      "   :  +- Range (0, 10, step=1, splits=1)\n",
      "   +- Range (0, 100, step=1, splits=1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b1 = spark.range(10)\n",
    "b2 = spark.range(100)\n",
    "\n",
    "b1.join(b2, b1.id >= b2.id).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a23b1b",
   "metadata": {},
   "source": [
    "This join is slow and this join will not work when either sides are big enough for broadcasting and you could see Out Of Memory exceptions.\n",
    "\n",
    "Works for both equi and non-equi joins Works for all join types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6aa47d",
   "metadata": {},
   "source": [
    "# Hints\n",
    "we can also give hint to spark to choose a certain type of join. 4 types of hints are:\n",
    "\n",
    "BROADCAST\n",
    "\n",
    "MERGE \n",
    "\n",
    "SHUFFLE_HASH and \n",
    "\n",
    "SHUFFLE_REPLICATE_NL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0386af95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [Id#440L, Name#441, Age#445L]\n",
      "   +- ShuffledHashJoin [Id#440L], [Id#444L], Inner, BuildRight\n",
      "      :- Exchange hashpartitioning(Id#440L, 200), ENSURE_REQUIREMENTS, [plan_id=1167]\n",
      "      :  +- Filter isnotnull(Id#440L)\n",
      "      :     +- Scan ExistingRDD[Id#440L,Name#441]\n",
      "      +- Exchange hashpartitioning(Id#444L, 200), ENSURE_REQUIREMENTS, [plan_id=1168]\n",
      "         +- Filter isnotnull(Id#444L)\n",
      "            +- Scan ExistingRDD[Id#444L,Age#445L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lets try to use the same data joined to demonstrate sort merge join\n",
    "\n",
    "dataf = spark.createDataFrame([(1,\"Shreyas\"),(2,\"Jayaram\")],[\"Id\",\"Name\"])\n",
    "datag = spark.createDataFrame([(1,25),(2,60)],[\"Id\",\"Age\"])\n",
    "\n",
    "dataf.join(datag.hint(\"shuffle_hash\"),['id'],'inner').explain()\n",
    "\n",
    "# we can see that although this was doing sort merge join previously, as we have given hint, it is performing shuffle hash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf33c6",
   "metadata": {},
   "source": [
    "Spark algorithm to choose join strategy\n",
    "\n",
    "If it is an equi-join, we first look at the join hints w.r.t. the following order:\n",
    "      //   1. broadcast hint: pick broadcast hash join if the join type is supported. If both sides\n",
    "      //      have the broadcast hints, choose the smaller side (based on stats) to broadcast.\n",
    "      //   2. sort merge hint: pick sort merge join if join keys are sortable.\n",
    "      //   3. shuffle hash hint: We pick shuffle hash join if the join type is supported. If both\n",
    "      //      sides have the shuffle hash hints, choose the smaller side (based on stats) as the\n",
    "      //      build side.\n",
    "      //   4. shuffle replicate NL hint: pick cartesian product if join type is inner like.\n",
    "      //\n",
    "      \n",
    "      \n",
    "If there is no hint or the hints are not applicable, we follow these rules one by one:\n",
    "      //   1. Pick broadcast hash join if one side is small enough to broadcast, and the join type\n",
    "      //      is supported. If both sides are small, choose the smaller side (based on stats)\n",
    "      //      to broadcast.\n",
    "      //   2. Pick shuffle hash join if one side is small enough to build local hash map, and is\n",
    "      //      much smaller than the other side, and `spark.sql.join.preferSortMergeJoin` is false.\n",
    "      //   3. Pick sort merge join if the join keys are sortable.\n",
    "      //   4. Pick cartesian product if join type is inner like.\n",
    "      //   5. Pick broadcast nested loop join as the final solution. It may OOM but we don't have\n",
    "      //      other choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8e521f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
