{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6bff08d",
   "metadata": {},
   "source": [
    "# Key-Value RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d148d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('example rdd').master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd55a4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'is', 'shreyas', 'is', 'name']\n",
      "[('my', 1), ('name', 1), ('is', 1), ('shreyas', 1), ('is', 1), ('name', 1)]\n"
     ]
    }
   ],
   "source": [
    "words = \"my name is shreyas is name\".split(\" \")\n",
    "rdd = spark.sparkContext.parallelize(words)\n",
    "print(rdd.collect())\n",
    "\n",
    "#lets try to create a key value rdd using the above rdd\n",
    "rdd1 = rdd.map(lambda word:(word,1))\n",
    "print(rdd1.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fa60e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('M', 'my'), ('N', 'name'), ('I', 'is'), ('S', 'shreyas'), ('I', 'is'), ('N', 'name')]\n",
      "PythonRDD[11] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "# spark also provides methods to create keys and values.\n",
    "# keyBY(func) - creates keys from values present in original rdd according to the function provided.\n",
    "\n",
    "rdd2 = rdd.keyBy(lambda word:word[0].upper())\n",
    "print(rdd2.collect())\n",
    "\n",
    "#if we want to change only values in the rdd we can use below function\n",
    "#mapValues() - Pass each value in the key-value pair RDD through a map function without changing the keys\n",
    "\n",
    "rdd3 = rdd2.mapValues(lambda word: word.upper())\n",
    "print(rdd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6190dd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M', 'N', 'I', 'S', 'I', 'N']\n",
      "['MY', 'NAME', 'IS', 'SHREYAS', 'IS', 'NAME']\n",
      "['NAME', 'NAME']\n"
     ]
    }
   ],
   "source": [
    "# to get all keys and values from a rdd\n",
    "\n",
    "print(rdd3.keys().collect())\n",
    "print(rdd3.values().collect())\n",
    "\n",
    "#to get value associated with a particular key\n",
    "print(rdd3.lookup('N'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54320ee8",
   "metadata": {},
   "source": [
    "# Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ff17eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'M': 1, 'N': 2, 'I': 2, 'S': 1})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#countByKey - You can count the number of elements for each key, collecting the results to a local Map\n",
    "\n",
    "rdd3.countByKey()\n",
    "display(rdd3.countByKey())\n",
    "\n",
    "# this creates a python dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166fd263",
   "metadata": {},
   "source": [
    "groupByKey() is the most frequently used wide transformation operation that involves shuffling of data across the executors when data is not partitioned on the Key. It takes key-value pairs (K, V) as an input, groups the values based on the key(K), and generates a dataset of KeyValueGroupedDataset(K, Iterable) pairs as an output.\n",
    "It’s a very expensive operation and consumes a lot of memory if the dataset is huge. Hence better to avoid it\n",
    "\n",
    "https://sparkbyexamples.com/spark/spark-groupbykey/\n",
    "\n",
    "reduceByKey() transformation is used to merge the values of each key using an associative reduce function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ed69fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('key2', <pyspark.resultiterable.ResultIterable object at 0x00000222E8AB4AD0>), ('key1', <pyspark.resultiterable.ResultIterable object at 0x00000222E8AB4B10>)]\n"
     ]
    }
   ],
   "source": [
    "rdd4=spark.sparkContext.parallelize(((\"key1\", 1), (\"key2\", 2), (\"key1\", 3), (\"key2\", 4)))\n",
    "#print(rdd.collect())\n",
    "\n",
    "rdd5= rdd4.groupByKey()\n",
    "\n",
    "print(rdd5.collect())\n",
    "# we can see that it gives a key and an iterable over values. We can apply other function using this iterable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21056934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('key2', 6), ('key1', 4)]\n"
     ]
    }
   ],
   "source": [
    "rdd6=rdd4.reduceByKey(lambda a,b:a+b)\n",
    "print(rdd6.collect())\n",
    "\n",
    "#as we can see here it is easier to use reducebykey. Here we easily got the sum of values for a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdb6e967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key2', (2, 200)), ('key1', (1, 100))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joins on key value RDD. combines values when keys match\n",
    "a_rdd = spark.sparkContext.parallelize(((\"key1\", 1), (\"key2\", 2), (\"key4\", 4)))\n",
    "b_rdd = spark.sparkContext.parallelize(((\"key1\", 100), (\"key2\", 200), (\"key3\", 300),))\n",
    "\n",
    "a_rdd.join(b_rdd).collect()\n",
    "\n",
    "#similarly we can perform fullOuterJoin, leftOuterJoin, rightOuterJoin, cartesian by replacing join with these names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a93cf40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('key1', 1), ('key1', 100)),\n",
       " (('key2', 2), ('key2', 200)),\n",
       " (('key4', 4), ('key3', 300))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zip - “zip” together two RDDs, assuming that they have the same length. \n",
    "#This creates a PairRDD. The two RDDs must have the same number of partitions as well as the same number of elements\n",
    "#this is like union\n",
    "a_rdd.zip(b_rdd).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c871f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d6d55b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
